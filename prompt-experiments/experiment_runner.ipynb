{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8d64fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from autoddg import AutoDDG\n",
    "from autoddg.utils import get_sample\n",
    "from autoddg.evaluation import BaseEvaluator\n",
    "from typing import Optional\n",
    "# --- Import custom files ---\n",
    "from prompts import ALL_RELATED_WORK_PROMPTS\n",
    "from utils import log_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91509d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM Config ---\n",
    "MODEL_CONFIG = {\n",
    "    \"base_url\": \"http://localhost:11434/v1\",\n",
    "    \"api_key\": \"ollama\",\n",
    "    \"model_name\": \"llama3\",\n",
    "}\n",
    "\n",
    "# --- Experiment Config ---\n",
    "DATASET_NAME = \"CODE-15%: a large scale annotated dataset of 12-lead ECGs\"\n",
    "DATA_FILE = \"../src/autoddg/related/data/code-15.csv\"\n",
    "PAPER_FILE = \"../src/autoddg/related/papers/code15.pdf\"\n",
    "RESULTS_FILE = \"autoddg_experiment_results.csv\"\n",
    "\n",
    "# --- Define Evaluation Class ---\n",
    "class Eval(BaseEvaluator):\n",
    "    def __init__(self, model_name: str = MODEL_CONFIG[\"model_name\"]):\n",
    "        client = OpenAI(\n",
    "            api_key=MODEL_CONFIG[\"api_key\"], \n",
    "            base_url=MODEL_CONFIG[\"base_url\"]\n",
    "        )\n",
    "        super().__init__(client=client, model_name=model_name)\n",
    "\n",
    "# Initialize Core Tools\n",
    "client = OpenAI(api_key=MODEL_CONFIG[\"api_key\"], base_url=MODEL_CONFIG[\"base_url\"])\n",
    "auto_ddg = AutoDDG(client=client, model_name=MODEL_CONFIG[\"model_name\"])\n",
    "auto_ddg.set_evaluator(Eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Data and Running Core Profiling ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/nlp_final/lib/python3.12/site-packages/datamart_profiler/core.py:199: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data = data.astype(object).fillna('').astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Loading and Core Profiling (Run Once)\n",
    "\n",
    "print(\"--- Loading Data and Running Core Profiling ---\")\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "sample_df, dataset_sample = get_sample(df, sample_size=100)\n",
    "\n",
    "basic_profile, structural_profile = auto_ddg.profile_dataframe(df)\n",
    "semantic_profile = auto_ddg.analyze_semantics(sample_df)\n",
    "data_topic = auto_ddg.generate_topic(DATASET_NAME, None, dataset_sample)\n",
    "\n",
    "print(\"Profiling Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6269a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define Prompts to Test (Selects from the imported dictionary)\n",
    "\n",
    "# Define which prompts you want to run for this experiment\n",
    "PROMPTS_TO_TEST = {\n",
    "    \"V1_Revised\": ALL_RELATED_WORK_PROMPTS[\"V1_Revised\"],\n",
    "    # \"V2_Aggressive\": ALL_RELATED_WORK_PROMPTS[\"V2_Aggressive\"],\n",
    "    \"V3_Hybrid\": ALL_RELATED_WORK_PROMPTS[\"V3_Hybrid\"]\n",
    "}\n",
    "print(f\"Testing {len(PROMPTS_TO_TEST)} related work prompts.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a247b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run and Log Baseline (Vanilla) Description\n",
    "\n",
    "print(\"\\n--- Running Baseline (Vanilla) Test ---\")\n",
    "prompt_baseline, description_baseline = auto_ddg.describe_dataset(\n",
    "    dataset_sample=dataset_sample,\n",
    "    dataset_profile=basic_profile,\n",
    "    use_profile=True,\n",
    "    semantic_profile=semantic_profile,\n",
    "    use_semantic_profile=True,\n",
    "    data_topic=data_topic,\n",
    "    use_topic=True,\n",
    "    use_related_profile=False  # Vanilla\n",
    ")\n",
    "\n",
    "baseline_scores = auto_ddg.evaluate_description(description_baseline)\n",
    "print(f\"Baseline Scores: {baseline_scores}\")\n",
    "\n",
    "# Log result using the new utility function (passing required file/dataset args)\n",
    "log_result(\n",
    "    prompt_name=\"N/A\", \n",
    "    description_type=\"Vanilla_AutoDDG\", \n",
    "    description=description_baseline, \n",
    "    raw_scores=baseline_scores,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    file_path=RESULTS_FILE\n",
    "    # related_profile is omitted (defaults to None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run and Log Augmented Descriptions for Multiple Prompts\n",
    "\n",
    "for prompt_name, extraction_prompt in PROMPTS_TO_TEST.items():\n",
    "    print(f\"\\n--- Running Augmented Test with Prompt: {prompt_name} ---\")\n",
    "    \n",
    "    # Step A: Analyze related work using the current prompt\n",
    "    related_profile = auto_ddg.analyze_related(\n",
    "        pdf_path=PAPER_FILE,\n",
    "        dataset_name=DATASET_NAME,\n",
    "        extraction_prompt=extraction_prompt,\n",
    "        max_pages=10\n",
    "    )\n",
    "    print(f\"Related Work Summary: {related_profile['summary'][:150]}...\")\n",
    "\n",
    "    # Step B: Generate description with the new related profile\n",
    "    prompt_augmented, description_augmented = auto_ddg.describe_dataset(\n",
    "        dataset_sample=dataset_sample,\n",
    "        dataset_profile=basic_profile,\n",
    "        use_profile=True,\n",
    "        semantic_profile=semantic_profile,\n",
    "        use_semantic_profile=True,\n",
    "        data_topic=data_topic,\n",
    "        use_topic=True,\n",
    "        related_profile=related_profile,\n",
    "        use_related_profile=True # Augmented\n",
    "    )\n",
    "    \n",
    "    # Step C: Evaluate and Log\n",
    "    augmented_scores = auto_ddg.evaluate_description(description_augmented)\n",
    "    print(f\"Augmented Scores ({prompt_name}): {augmented_scores}\")\n",
    "    \n",
    "    log_result(\n",
    "        prompt_name=prompt_name, \n",
    "        description_type=\"Augmented_AutoDDG\", \n",
    "        description=description_augmented, \n",
    "        raw_scores=augmented_scores,\n",
    "        dataset_name=DATASET_NAME,\n",
    "        file_path=RESULTS_FILE,\n",
    "        related_profile=related_profile # Now logs the profile!\n",
    "    )\n",
    "    \n",
    "print(\"\\nAll experiments complete. Results saved to:\", RESULTS_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
