dataset_name,zenodo_record_id,description,best_paper_title,best_paper_doi,best_paper_citations,pdf_path
The Global Carbon Project's fossil CO2 emissions dataset,5569235,"<p>The <a href=""https://www.globalcarbonproject.org/"">Global Carbon Project</a> (GCP) has been publishing estimates of global and national fossil CO<sub>2</sub> emissions since 2001. In the first instance these were simple republications of data from another source, but over subsequent years refinements have been made in response to feedback and identification of inaccuracies. In this article we describe the history of this process leading up to the methodology used in the 2021 release of the GCP&rsquo;s fossil CO<sub>2</sub> dataset.</p>  <p>The fossil CO<sub>2</sub> emissions dataset is included in both its standard, absolute form, and per capita, with associated metadata files in JSON format. A file indicating the source(s) of each data point is also provided (this is a work in progress).</p>",Global Carbon Budget 2022,https://doi.org/10.5194/essd-14-4811-2022,1565,pdfs/Global_Carbon_Budget_2022.pdf
Silva 138.1 prokaryotic SSU taxonomic training data formatted for DADA2,4587955,"<p>These training fasta files are derived from the Silva Project&#39;s version 138.1 release and formatted for use with DADA2. These files are intended for use in classifying prokaryotic 16S sequencing data and are not appropriate for classifying eukaryotic ASVs.</p>  <p>See&nbsp;https://benjjneb.github.io/dada2/training.html for information about DADA2 reference databases and&nbsp;https://www.arb-silva.de/documentation/release-138.1/ for database and citation information for Silva 138.1. The Silva 138.1 database is licensed under Creative Commons Attribution 4.0 (CC-BY 4.0); see file &quot;SILVA_LICENSE.txt&quot;. These fasta database&nbsp;files were generated and checked for consistency using the R markdown documents&nbsp;in the silva-138.1 folder in&nbsp;https://zenodo.org/record/4587946.</p>  <p>If you use these files, please cite one or both of the Silva references below (or at the above link) and the DADA2 paper (reference below). I also recommend&nbsp;citing or linking to the Zenodo record for this specific version in your Methods or published source code to record&nbsp;the specific taxonomic database files used in your analysis.</p>  <p><strong>NOTE:</strong> These database files have a known problem in 3/895 families and 59/3936 genera. See https://github.com/mikemc/dada2-reference-databases/blob/main/silva-138.1/v1/bad-taxa.csv for a list of affected taxa and https://github.com/benjjneb/dada2/issues/1293 for more information.</p>",Microbial growth under drought is confined to distinct taxa and modified by potential future climate conditions,https://doi.org/10.1038/s41467-023-41524-y,85,pdfs/Microbial_growth_under_drought_is_confined_to_distinct_taxa_and_modified_by_pote.pdf
The FluPRINT database,3222451,"<p><strong>What is the FluPRINT database?</strong></p>  <p>The FluPRINT represents fully integrated and normalized immunology measurements from eight clinical studies taken from 740 individuals undergoing influenza vaccination with inactivated or live attenuated seasonal influenza vaccines from 2007 to 2015 at the Stanford Human Immune Monitoring Center.</p>  <p>The FluPRINT dataset contains information on more than 3,000 parameters measured using mass cytometry, flow cytometry, phosphorylation-specific cytometry, multiplex cytokine assays, clinical lab tests (hormones and complete blood count), serological profiling and virological tests. In the dataset, vaccine protection is measured using a hemagglutination inhibition (HAI) assay, and following FDA guidelines individuals are marked as high or low responders depending on the HAI antibody titers after vaccination.</p>  <p><strong>Want to know more?</strong></p>  <p>To understand how the FluPRINT dataset was generated and validated, and how to use it, please refer to our open-access paper published in Scientific Data journal:</p>  <p>Tomic, A., Tomic, I., Dekker, C.L. <em>et al.</em> The FluPRINT dataset, a multidimensional analysis of the influenza vaccine imprint on the immune system. <em>Sci Data</em> <strong>6, </strong>214 (2019). https://doi.org/10.1038/s41597-019-0213-4</p>  <p>For additional exploration, please check out the project&rsquo;s website: <a href=""www.fluprint.com"">www.fluprint.com</a>, where you can also explore the FluPRINT dataset on the following link: <a href=""https://fluprint.com/#/database-access"">https://fluprint.com/#/database-access</a>.</p>  <p>If you want to host your own FluPRINT database, please follow our instructions provided on the Github repository: <a href=""https://github.com/LogIN-/fluprint"">https://github.com/LogIN-/fluprint</a>.</p>  <p><strong>How to use FluPRINT?</strong></p>  <p>Here, you can download the entire FluPRINT database export as an SQL file, or as a CSV file. Additionally, we included the file with the SQL query to obtain those files.</p>  <p>Files are provided in two formats: zip and 7zip (7z). 7zip is a free and open-source file archiver available for download here: <a href=""https://www.7-zip.org"">https://www.7-zip.org</a>.</p>  <p>In the FluPRINT database, there are 4 tables:<strong> <em>donor, donor_visits, experimental_data, </em></strong>and<strong><em> medical_history</em></strong><em>.</em><br> The exact description of each table is available in <a href=""https://www.biorxiv.org/content/10.1101/564062v1"">the FluPRINT paper</a>.</p>  <p>Briefly, in the table<strong> <em>donor</em></strong>, each row represents an individual with information about the clinical study in which an individual was enrolled (study ID and study internal ID), gender, and race. The second table, named <strong><em>donor_visits </em></strong>describes information about the donor&rsquo;s age, cytomegalovirus (CMV) and Epstein-Barr virus (EBV) status, Body Mass Index (BMI), and vaccine received on each clinical visit. Information about vaccine outcome is available as geometric mean titers (geo_mean), the difference in the geometric mean titers before and after vaccination (delta_geo_mean), and the difference for each vaccine strain (delta_single). In the last field, each individual is classified as a high and low responder (vaccine_resp). On each visit, samples were analyzed and information about which assays were performed (assay field) and value of the measured analytes (units and data) are stored in the <strong><em>experimental_data</em></strong> table. Finally, the <strong><em>medical_history</em></strong> table describes information connected with each clinical visit about the usage of statins (statin_use) and if influenza vaccine was received in the past (influenza vaccine history), if yes, how many times (total_vaccines_received). Also, we provide information on which type of influenza vaccine was received in the previous years (1 to 5 years prior to enrolment in the clinical study). Lastly, information about influenza infection history and influenza-related hospitalization is provided.</p>  <p><strong>How to cite FluPRINT?</strong></p>  <p><em>If you use FluPRINT in an academic publication, please use the following citation:&nbsp;</em></p>  <p>Tomic, A., Tomic, I., Dekker, C.L. <em>et al.</em> The FluPRINT dataset, a multidimensional analysis of the influenza vaccine imprint on the immune system. <em>Sci Data</em> <strong>6, </strong>214 (2019). https://doi.org/10.1038/s41597-019-0213-4</p>  <p><strong>Contact Information</strong></p>  <p>If you are interested to find out more about the FluPRINT, or if you experience any problems with downloading files, please contact us at <a href=""mailto:info@adrianatomic.com"">info@adrianatomic.com</a>.</p>",Mucosal vaccines â€” fortifying the frontiers,https://doi.org/10.1038/s41577-021-00583-2,640,pdfs/Mucosal_vaccines___fortifying_the_frontiers.pdf
Automated measurement of fetal head circumference,1322001,"<p>For more information about this dataset go to:&nbsp;<a href=""https://hc18.grand-challenge.org/"">https://hc18.grand-challenge.org/</a></p>","Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past, Present and Future",https://doi.org/10.3390/s21144758,211,
Crowd-sourced Fitbit datasets 03.12.2016-05.12.2016,53894,"<p>These datasets were generated by respondents to a distributed survey via&nbsp;Amazon Mechanical Turk between 03.12.2016-05.12.2016. &nbsp;Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring.&nbsp;Individual reports can be parsed by export session ID (column A) or timestamp (column B). &nbsp;Variation between output represents use of different types of Fitbit trackers and individual&nbsp;tracking behaviors / preferences. &nbsp;<br /> &nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>",Cardiologist-level interpretable knowledge-fused deep neural network for automatic arrhythmia diagnosis,https://doi.org/10.1038/s43856-024-00464-4,31,pdfs/Cardiologist-level_interpretable_knowledge-fused_deep_neural_network_for_automat.pdf
CODE-15\%: a large scale annotated dataset of 12-lead ECGs,4916206,"<p>A dataset of 12-lead ECGs with annotations. The dataset contains 345 779 exams from 233 770 patients. It was obtained through stratified sampling from the CODE dataset ( 15% of the patients). The data was collected by the Telehealth Network of Minas Gerais in the period between 2010 and 2016.</p> <p>This repository&nbsp;contains the files `exams.csv` and the files `exams_part{i}.zip` for i = 0,&nbsp;1, 2, ... 17.&nbsp;</p> <ul> <li>""exams.csv"": is a comma-separated values (csv) file&nbsp;containing the columns <ul> <li>""exam_id"": id used for identifying the exam;</li> <li>""age"": patient age in years at the moment of the exam;</li> <li>""is_male"": true if the patient is male;</li> <li>""nn_predicted_age"": age predicted by a neural network to the patient. As described in the paper&nbsp;""Deep neural network estimated electrocardiographic-age as a mortality predictor"" bellow.</li> <li>""1dAVb"": Whether or not the patient has 1st degree AV block;</li> <li>""RBBB"":&nbsp;Whether or not the patient has right bundle branch block;</li> <li>""LBBB"": Whether or not the patient has left&nbsp;bundle branch block;</li> <li>""SB"": Whether or not the patient has sinus bradycardia;</li> <li>""AF"":&nbsp;Whether or not the patient has atrial fibrillation;</li> <li>""ST"": Whether or not the patient has sinus tachycardia;</li> <li>""patient_id"": id used for identifying the patient;</li> <li>""normal_ecg"": True if automatic annotation system say it is a normal ECG;</li> <li>""death"": true if the patient dies in the follow-up time. This data is available only in the first exam of the patient. Other exams will have this as an empty field;</li> <li>""timey"": if the patient dies it is the time to the death of the patient. If not, it is the follow-up time.&nbsp;This data is available only in the first exam of the patient. Other exams will have this as an empty field;</li> <li>""trace_file"": identify in which hdf5 file the file corresponding to this patient is located.</li> </ul> </li> <li>""exams_part{i}.hdf5"": The HDF5 file containing two datasets&nbsp;named `tracings` and other named `exam_id`. The `exam_id` is a tensor of dimension `(N,)` containing the exam id (the same as in the csv file) and the&nbsp;dataset `tracings` is a `(N, 4096, 12)` tensor containing the ECG tracings in the same order. The first dimension corresponds to the different exams; the second dimension corresponds to the 4096 signal samples; the third dimension to the 12 different leads of the ECG exams in the following order: `{DI, DII, DIII, AVR, AVL, AVF, V1, V2, V3, V4, V5, V6}`. The signals are sampled at 400 Hz. Some signals originally have a duration of 10 seconds (10 * 400 = 4000 samples) and others of 7 seconds (7 * 400 = 2800 samples). In order to make them all have the same size (4096 samples), we fill them with zeros on both sizes. For instance, for a 7 seconds ECG signal with 2800 samples we include 648 samples at the beginning and 648 samples at the end, yielding 4096 samples that are then saved in the hdf5 dataset.&nbsp; <p>In python, one can read this file using <a href=""https://www.h5py.org/"">h5py</a>.<br>```python<br>import h5py</p> <p>f = h5py.File(path_to_file, 'r')<br># Get ids<br>traces_ids = np.array(self.f['id_exam'])<br>x = f['signal']<br>```<br>The `signal` dataset is too large to fit in memory, so don't convert it to a numpy array all at once.<br>It is possible to access a chunk of it using: ``x[start:end, :, :]``.</p> </li> </ul> <p>The CODE dataset&nbsp;was collected by&nbsp;the Telehealth Network of Minas Gerais (TNMG) in the period&nbsp;between 2010 and 2016. TNMG is&nbsp;a public telehealth system assisting 811 out of the 853 municipalities in the state of Minas Gerais, Brazil. The dataset is described</p> <p>Ribeiro, Ant&ocirc;nio H., Manoel Horta Ribeiro, Gabriela M. M. Paix&atilde;o, Derick M. Oliveira, Paulo R. Gomes, J&eacute;ssica A. Canazart, Milton P. S. Ferreira, et al. &ldquo;Automatic Diagnosis of the 12-Lead ECG Using a Deep Neural Network.&rdquo; <em>Nature Communications</em> 11, no. 1 (2020): 1760.&nbsp;https://doi.org/10.1038/s41467-020-15432-4</p> <p>The&nbsp;CODE 15% dataset is obtained from stratified sampling from the CODE dataset. This subset of the code dataset is described in and used for assessing model performance:<br>""Deep neural network estimated electrocardiographic-age as a mortality predictor""<br>Emilly M Lima, Ant&ocirc;nio H Ribeiro, Gabriela MM Paix&atilde;o, Manoel Horta Ribeiro,&nbsp;Marcelo M Pinto Filho, Paulo R Gomes, Derick M Oliveira,&nbsp;Ester C Sabino, Bruce B Duncan, Luana Giatti, Sandhi M Barreto, Wagner Meira Jr, Thomas B Sch&ouml;n, Antonio Luiz P Ribeiro. MedRXiv (2021) https://www.doi.org/10.1101/2021.02.19.21251232<br><br>The companion code for reproducing&nbsp;the experiments in the two papers described above can be found, respectively, in:<br>- <a href=""http://github.com/antonior92/automatic-ecg-diagnosis"">https://github.com/antonior92/automatic-ecg-diagnosis</a>; and in,<br>-&nbsp;<a href=""http://github.com/antonior92/ecg-age-prediction"">https://github.com/antonior92/ecg-age-prediction</a>.<br><br>Note about authorship:&nbsp;Ant&ocirc;nio H. Ribeiro,&nbsp;Emilly M.&nbsp;Lima and&nbsp;Gabriela M.M. Paix&atilde;o&nbsp;contributed equally&nbsp;to this work.</p>",Cardiologist-level interpretable knowledge-fused deep neural network for automatic arrhythmia diagnosis,https://doi.org/10.1038/s43856-024-00464-4,31,pdfs/Cardiologist-level_interpretable_knowledge-fused_deep_neural_network_for_automat.pdf
FCCdb: Food Contact Chemicals database. Version 5.0,4296944,"<p>The Food Contact Chemicals database (FCCdb) is&nbsp;a compilation of information on intentionally added food contact chemicals, extracted from publicly available sources such as legislation on food contact materials and&nbsp;industry inventories for different types of food contact materials. Where available, information from a few selected sources&nbsp;on hazardous properties and commercial use has been included as well. Further details on the information sources used are given in the READ ME worksheet of the excel file. The FCCdb intends to provide an overview of the diversity of food contact chemicals and their hazardous properties. Further details on the compilation and analysis of this dataseta can be found in the manuscript &quot;Overview of intentionally used food contact chemicals and their hazards,&quot; by Ksenia J. Groh, Birgit Geueke, Olwenn Martin, Maricel Maffini, and Jane Muncke, published in&nbsp;<em>Environmental International&nbsp;</em>on November 30, 2020 (DOI 10.1016/j.envint.2020.106225).</p>",The NORMAN Suspect List Exchange (NORMAN-SLE): facilitating European and worldwide collaboration on suspect screening in high resolution mass spectrometry,https://doi.org/10.1186/s12302-022-00680-6,137,pdfs/The_NORMAN_Suspect_List_Exchange__NORMAN-SLE___facilitating_European_and_worldwi.pdf
Nonhuman Primate Reaching with Multichannel Sensorimotor Cortex Electrophysiology,3854034,"<p><strong>General Description.</strong> This&nbsp;dataset consists of:</p> <ol> <li>The threshold crossing times of extracellularly and simultaneously&nbsp;recorded spikes, sorted into units (up to five, including a ""hash"" unit), along with sorted waveform snippets, and,</li> <li>The x,y position of the fingertip of the reaching hand and the x,y position of reaching targets (both sampled at 250 Hz).</li> </ol> <p>The behavioral task was to make self-paced reaches to targets arranged in a grid (e.g. 8x8) without gaps or pre-movement delay intervals. One monkey reached with the right arm (recordings made in the left hemisphere); The other reached with the left arm (right hemisphere). In some sessions recordings were made from both M1 and S1 arrays (192 channels); in most sessions M1 recordings were made alone (96 channels).</p> <p>Data from two primate subjects are included: 37 sessions from monkey 1 (""Indy"",&nbsp;spanning about 10 months) and 10 sessions from monkey 2 (""Loco"",&nbsp;spanning about 1 month), for a total of ~ 20,000 reaches and 6,500 reaches from monkeys 1 and 2, respectively.</p> <p><strong>Possible uses.&nbsp;</strong>These data are ideal for training BCI decoders, in particular because they are not segmented into trials.&nbsp;We expect that the dataset will be valuable for researchers who wish to design improved models of sensorimotor cortical spiking&nbsp;or provide an equal footing for comparing&nbsp;different BCI decoders. Other uses could include analyses of the statistics of arm kinematics, spike noise-correlations or signal-correlations, or for exploring the stability or variability of extracellular recording over sessions.</p> <p><strong>Variable names. </strong>Each file contains data in the following format. In the below, <em>n</em> refers to the number of recording channels, <em>u</em> refers to the number of sorted units, and&nbsp;<em>k</em> refers to the number of samples.</p> <ul> <li>chan_names -&nbsp;n&nbsp;x&nbsp;1 <ul> <li>A cell array of channel identifier strings, e.g. ""<em>M1&nbsp;001</em>"".</li> </ul> </li> <li>cursor_pos -&nbsp;k&nbsp;x&nbsp;2 <ul> <li>The position of the cursor in Cartesian coordinates (x, y),&nbsp;mm.</li> </ul> </li> <li>finger_pos - k&nbsp;x&nbsp;3 <em>or </em>k x 6 <ul> <li>The position of the working fingertip in Cartesian coordinates (z, -x, -y), as reported by the hand tracker&nbsp;in&nbsp;cm. Thus&nbsp;the cursor position is an affine&nbsp;transformation of fingertip position using the following matrix:<br>\(\begin{pmatrix} 0 &amp; 0 \\ -10 &amp; 0 \\ 0 &amp; -10 \end{pmatrix}\)<br>Note that for some sessions finger_pos includes the orientation of the sensor as well; the full state is&nbsp;thus: (z, -x, -y, azimuth, elevation, roll).</li> </ul> </li> <li>target_pos -&nbsp;k x 2 <ul> <li>The position of the target in Cartesian coordinates (x, y), mm.</li> </ul> </li> <li>t - k x 1 <ul> <li>The timestamp corresponding to each sample of the cursor_pos, finger_pos, and target_pos, seconds.</li> </ul> </li> <li>spikes - n&nbsp;x u <ul> <li>A cell array of spike event vectors.&nbsp;Each element in the cell array&nbsp;is a vector of spike event timestamps,&nbsp;in seconds.&nbsp;The first unit (<em>u</em>1) is the ""unsorted"" unit, meaning it contains the threshold crossings which remained after the spikes on that channel were sorted into other units (<em>u</em>2, <em>u</em>3,&nbsp;etc.) For some sessions spikes were sorted into up to 2 units (i.e. <em>u</em>=3);&nbsp;for others, 4&nbsp;units (<em>u</em>=5).</li> </ul> </li> <li>wf - n&nbsp;x u <ul> <li>A cell array of spike event waveform ""snippets"". Each element in the cell array is a matrix of spike event waveforms.&nbsp;Each waveform corresponds to a timestamp in ""spikes"". Waveform samples are in microvolts.</li> </ul> </li> </ul> <p><strong>Decoder Results.</strong>&nbsp;These data were used to fit decoder models, as reported in Makin, et al [1]. To aid comparisons to other decoders, we include performance summaries (for each session, decoder, bin-width, etc.) in the file <em>refh_results.csv</em>, containing the following columns:</p> <ul> <li>session - a session identifier, e.g. ""indy_20160407_02""</li> <li>monkey - one of, ""indy"" or ""loco""</li> <li>num_neurons - total number of features used in the decoder</li> <li>num_training_samples - number of samples (at the specified bin-width) used to train the decoder (sequential,&nbsp;from file start)</li> <li>num_testing_samples - number of samples used to evaluate the decoder (sequential, until file end)</li> <li>kinematic_axis - one of, ""posx"", ""posy"", ""velx"", ""vely"", ""accx"" or ""accy""</li> <li>bin_width - one of, ""16"", ""32"", ""64"" or ""128""</li> <li>decoder - one of, ""regression"", ""KF_observed"", ""KF_static"", ""KF_dynamic"", ""UKF"", ""rEFH_static"" or ""rEFH_dynamic""</li> <li>rsq - coefficient of determination, R2</li> <li>snr - Signal to noise ratio, SNR := -10 log10(1 - R2)</li> </ul> <p><strong>Videos. </strong>For some sessions, we recorded screencasts of the stimulus presentation display using a dedicated hardware video grabber. These screencasts are thus a&nbsp;faithful representation of the stimuli and feedback presented to the monkey and are&nbsp;available for the following sessions:</p> <ul> <li><a href=""https://youtu.be/bPkpdpm03z8"">indy_20160921_01</a></li> <li><a href=""https://youtu.be/B02z6w4c3yk"">indy_20160930_02</a></li> <li><a href=""https://youtu.be/S640zzIKJs8"">indy_20160930_05</a></li> <li><a href=""https://youtu.be/tRoe84E0AzA"">indy_20161005_06</a></li> <li><a href=""https://youtu.be/hNZlBa516jM"">indy_20161006_02</a></li> <li><a href=""https://youtu.be/L6GKwI2u1Es"">indy_20161007_02</a></li> <li><a href=""https://youtu.be/eV1joYU5vt0"">indy_20161011_03</a></li> <li><a href=""https://youtu.be/4LM_gKt2cYg"">indy_20161013_03</a></li> <li><a href=""https://youtu.be/GLGrKHgf-zw"">indy_20161014_04</a></li> <li><a href=""https://youtu.be/6aPrv8HEPGQ"">indy_20161017_02</a></li> </ul> <p><strong>Supplements. </strong>The raw broadband neural recordings that the spike trains in this dataset were extracted from are available for the following sessions:</p> <ul> <li>indy_20160622_01: <a href=""https://doi.org/10.5281/zenodo.1488440"">doi:10.5281/zenodo.1488440</a></li> <li>indy_20160624_03: <a href=""https://doi.org/10.5281/zenodo.1486147"">doi:10.5281/zenodo.1486147</a></li> <li>indy_20160627_01: <a href=""https://doi.org/10.5281/zenodo.1484824"">doi:10.5281/zenodo.1484824</a></li> <li>indy_20160630_01: <a href=""https://doi.org/10.5281/zenodo.1473703"">doi:10.5281/zenodo.1473703</a></li> <li>indy_20160915_01: <a href=""https://doi.org/10.5281/zenodo.1467953"">doi:10.5281/zenodo.1467953</a></li> <li>indy_20160916_01: <a href=""https://doi.org/10.5281/zenodo.1467050"">doi:10.5281/zenodo.1467050</a></li> <li>indy_20160921_01: <a href=""https://doi.org/10.5281/zenodo.1451793"">doi:10.5281/zenodo.1451793</a></li> <li>indy_20160927_04: <a href=""https://doi.org/10.5281/zenodo.1433942"">doi:10.5281/zenodo.1433942</a></li> <li>indy_20160927_06: <a href=""https://doi.org/10.5281/zenodo.1432818"">doi:10.5281/zenodo.1432818</a></li> <li>indy_20160930_02: <a href=""https://doi.org/10.5281/zenodo.1421880"">doi:10.5281/zenodo.1421880</a></li> <li>indy_20160930_05: <a href=""https://doi.org/10.5281/zenodo.1421310"">doi:10.5281/zenodo.1421310</a></li> <li>indy_20161005_06: <a href=""https://doi.org/10.5281/zenodo.1419774"">doi:10.5281/zenodo.1419774</a></li> <li>indy_20161006_02: <a href=""https://doi.org/10.5281/zenodo.1419172"">doi:10.5281/zenodo.1419172</a></li> <li>indy_20161007_02: <a href=""https://doi.org/10.5281/zenodo.1413592"">doi:10.5281/zenodo.1413592</a></li> <li>indy_20161011_03: <a href=""https://doi.org/10.5281/zenodo.1412635"">doi:10.5281/zenodo.1412635</a></li> <li>indy_20161013_03: <a href=""https://doi.org/10.5281/zenodo.1412094"">doi:10.5281/zenodo.1412094</a></li> <li>indy_20161014_04: <a href=""https://doi.org/10.5281/zenodo.1411978"">doi:10.5281/zenodo.1411978</a></li> <li>indy_20161017_02: <a href=""https://doi.org/10.5281/zenodo.1411882"">doi:10.5281/zenodo.1411882</a></li> <li>indy_20161024_03: <a href=""https://doi.org/10.5281/zenodo.1411474"">doi:10.5281/zenodo.1411474</a></li> <li>indy_20161025_04: <a href=""https://doi.org/10.5281/zenodo.1410423"">doi:10.5281/zenodo.1410423</a></li> <li>indy_20161026_03:&nbsp;<a href=""https://doi.org/10.5281/zenodo.1321264"">doi:10.5281/zenodo.1321264</a></li> <li>indy_20161027_03:&nbsp;<a href=""https://doi.org/10.5281/zenodo.1321256"">doi:10.5281/zenodo.1321256</a></li> <li>indy_20161206_02:&nbsp;<a href=""https://doi.org/10.5281/zenodo.1303720"">doi:10.5281/zenodo.1303720</a></li> <li>indy_20161207_02:&nbsp;<a href=""https://doi.org/10.5281/zenodo.1302866"">doi:10.5281/zenodo.1302866</a></li> <li>indy_20161212_02: <a href=""https://doi.org/10.5281/zenodo.1302832"">doi:10.5281/zenodo.1302832</a></li> <li>indy_20161220_02:&nbsp;<a href=""https://doi.org/10.5281/zenodo.1301045"">doi:10.5281/zenodo.1301045</a></li> <li>indy_20170123_02:&nbsp;<a href=""https://doi.org/10.5281/zenodo.1167965"">doi:10.5281/zenodo.1167965</a></li> <li>indy_20170124_01:&nbsp;<a href=""https://doi.org/10.5281/zenodo.1163026"">doi:10.5281/zenodo.1163026</a></li> <li>indy_20170127_03:&nbsp;<a href=""https://doi.org/10.5281/zenodo.1161225"">doi:10.5281/zenodo.1161225</a></li> <li>indy_20170131_02:&nbsp;<a href=""https://doi.org/10.5281/zenodo.854733"">doi:10.5281/zenodo.854733</a></li> </ul> <p><strong>Contact &nbsp;Information.</strong>&nbsp;We would be delighted to hear from you if you find this dataset valuable, especially if it leads to publication.&nbsp;Corresponding author:&nbsp;J. E. O'Doherty &lt;joeyo@neuroengineer.com&gt;.</p> <p><strong>Citation.</strong></p> <p>@misc{ODoherty:2017, &nbsp;author = {O'{D}oherty, Joseph E. and Cardoso, Mariana M. B. and Makin, Joseph G. and Sabes, Philip N.}, &nbsp;title &nbsp;= {Nonhuman Primate Reaching with Multichannel Sensorimotor Cortex electrophysiology}, &nbsp;doi &nbsp; &nbsp;= {10.5281/zenodo.788569}, &nbsp;url &nbsp; &nbsp;= {https://doi.org/10.5281/zenodo.788569}, &nbsp;month &nbsp;= may, &nbsp;year &nbsp; = {2017} }</p> <p><strong>Publications making use of this dataset.</strong></p> <ol> <li>Makin, J. G., O'Doherty, J. E., Cardoso, M. M. B. &amp; Sabes, P. N. (2018). Superior arm-movement decoding from cortex with a new, unsupervised-learning algorithm. <em>J Neural Eng.</em>&nbsp;15(2): 026010. <a href=""https://doi.org/10.1088/1741-2552/aa9e95"">doi:10.1088/1741-2552/aa9e95</a></li> <li>Ahmadi, N., Constandinou, T. G., &amp; Bouganis, C.-S. (2018). Spike Rate Estimation Using Bayesian Adaptive Kernel Smoother (BAKS) and Its Application to Brain Machine Interfaces.&nbsp;<em>2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em>, Honolulu, HI, USA, 2018, pp. 2547-2550. <a href=""https://doi.org/10.1109/EMBC.2018.8512830"">doi:10.1109/EMBC.2018.8512830</a></li> <li>Balasubramanian,&nbsp;M., Ruiz,&nbsp;T., Cook,&nbsp;B.,&nbsp;Bhattacharyya,&nbsp;S.,&nbsp;Prabhat,&nbsp;Shrivastava,&nbsp;A.&nbsp;&amp;&nbsp;Bouchard&nbsp;K.&nbsp;(2018).&nbsp;Optimizing the Union of Intersections LASSO (UoILASSO) and Vector Autoregressive (UoIVAR) Algorithms for Improved Statistical Estimation at Scale. <em>arXiv Preprint.</em>&nbsp;<a href=""https://arxiv.org/abs/1808.06992"">arXiv:1808.06992</a></li> <li>Sachdeva, P. S.,&nbsp;Bhattacharyya, S., &amp;&nbsp;Bouchard, K. E. (2019). Sparse, Predictive, and Interpretable Functional Connectomics with UoILasso,&nbsp;<em>41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em>, Berlin, Germany, pp. 1965-1968.&nbsp;<a href=""https://doi.org/10.1109/EMBC.2019.8856316"">doi:10.1109/EMBC.2019.8856316</a></li> <li>Ahmadi, N., Constandinou, T. G., &amp; Bouganis, C.-S. (2019). End-to-End Hand Kinematic Decoding from LFPs Using Temporal Convolutional Network. <em>2019 IEEE Biomedical Circuits and Systems Conference (BioCAS),&nbsp;</em>Nara, Japan, pp. 1-4.&nbsp;<a href=""https://doi.org/10.1109/biocas.2019.8919131"">doi:10.1109/biocas.2019.8919131</a></li> <li>Bose, S. K.,&nbsp;Acharya, J., &amp;&nbsp;Basu, A. (2019).&nbsp;Is my Neural Network Neuromorphic? Taxonomy, Recent Trends and Future Directions in Neuromorphic Engineering.&nbsp;<em>2019 53rd Asilomar Conference on Signals, Systems, and Computers</em>, Pacific Grove, CA, USA, pp. 1522-1527. <a href=""https://doi.org/10.1109/IEEECONF44664.2019.9048891"">doi:10.1109/IEEECONF44664.2019.9048891</a></li> <li>Shaikh, S., So, R.,&nbsp;Sibindi, T., Libedinsky, C., &amp; Basu, A. (2019).&nbsp;Towards Intelligent Intra-cortical BMI (i2BMI): Low-power Neuromorphic Decoders that outperform Kalman Filters.&nbsp;<em>bioRxiv Preprint.</em> 772988.&nbsp;<a href=""https://doi.org/10.1101/772988"">doi:10.1101/772988</a></li> <li>Keshtkaran, M. R.,&nbsp;&amp;&nbsp;Pandarinath, C. (2019).&nbsp;<a href=""https://papers.nips.cc/paper/9722-enabling-hyperparameter-optimization-in-sequential-autoencoders-for-spiking-neural-data"">Enabling hyperparameter optimization in sequential autoencoders for spiking neural data.</a> <em>Advances in Neural Information Processing Systems (NeurIPS)&nbsp;32.</em></li> <li>Clark, D. G., Livezey, J. A., &amp; Bouchard, K. E. (2019).&nbsp;Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis. <em>arXiv Preprint.</em>&nbsp;<a href=""https://arxiv.org/abs/1905.09944"">arXiv:1905.09944</a></li> <li>Shaikh, S., So, R., Sibindi, T., Libedinsky, C., &amp; Basu, A. (2019). Towards Intelligent Intracortical BMI (i2BMI): Low-Power Neuromorphic Decoders That Outperform Kalman Filters. <em>IEEE Transactions on Biomedical Circuits and Systems</em>. 13(6): 1615-1624. <a href=""https://doi.org/10.1109/TBCAS.2019.2944486"">doi:10.1109/TBCAS.2019.2944486</a></li> <li>Ahmadi, N., Constandinou, T. G., &amp; Bouganis, C.-S. (2019). Decoding Hand Kinematics from Local Field Potentials Using Long Short-Term Memory (LSTM) Network.&nbsp;<em>arXiv Preprint.</em>&nbsp;<a href=""https://arxiv.org/abs/1901.00708"">arXiv:1901.00708</a></li> <li>Balasubramanian,&nbsp;M., Ruiz,&nbsp;T., Cook,&nbsp;B.,&nbsp;Prabhat,&nbsp;Bhattacharyya,&nbsp;S.,&nbsp;Shrivastava,&nbsp;A.&nbsp;&amp;&nbsp;Bouchard&nbsp;K. (2020).&nbsp;Scaling of Union of Intersections for Inference of Granger Causal Networks from Observational Data.&nbsp;<em>Proceeding of the 34th IEEE International Parallel &amp; Distributed Processing Symposium (IPDPS).&nbsp;</em>New Orleans, LA, USA,&nbsp;pp. 264-273.&nbsp;<a href=""https://doi.org/10.1109/IPDPS47924.2020.00036"">doi: 10.1109/IPDPS47924.2020.00036</a></li> <li>Bose, S. K., Acharya, J. &amp; Basu, A. (2020). Is my neural network neuromorphic? Taxonomy, recent trends and future directions in neuromorphic engineering. <em>arXiv Preprint</em>.&nbsp;<a href=""https://arxiv.org/abs/2002.11945"">arXiv:2002.11945</a></li> <li>Ahmadi, N., Constandinou, T. G., &amp; Bouganis, C.-S. (2020).&nbsp;Inferring entire spiking activity from local field potentials with deep learning. <em>bioRxiv Preprint.</em>&nbsp;2020.05.02.074104.&nbsp;<a href=""https://doi.org/10.1101/2020.05.02.074104"">doi:10.1101/2020.05.02.074104</a></li> <li>Ahmadi, N.,&nbsp;Constandinou, T.&nbsp;G., &amp; Bouganis, C.-S.&nbsp;(2020).&nbsp;Improved Spike-based Brain-Machine Interface Using Bayesian Adaptive Kernel Smoother and Deep Learning. <em>TechRxiv Preprint.</em>&nbsp;<a href=""https://doi.org/10.36227/techrxiv.12383600.v1"">doi:10.36227/techrxiv.12383600.v1</a></li> <li>Ahmadi, N.,&nbsp;Constandinou, T.&nbsp;G., &amp; Bouganis, C.-S.&nbsp;(2020).&nbsp;Robust and accurate decoding of hand kinematics from entire spiking activity using deep learning.&nbsp;<em>bioRxiv Preprint.</em> 2020.05.07.083063&nbsp;<a href=""https://doi.org/10.1101/2020.05.07.083063"">doi:10.1101/2020.05.07.083063</a></li> <li>Ahmadi, N.,&nbsp;Constandinou, T. G., &amp; Bouganis. C.-S. (2020).&nbsp;Impact of referencing scheme on decoding performance of LFP-based brain-machine interface.&nbsp;<em>bioRxiv Preprint. </em>2020.05.03.075218 <a href=""https://doi.org/10.1101/2020.05.03.075218"">doi:10.1101/2020.05.03.075218</a></li> <li>Ahmadi, N., Constandinou, T. &amp; Bouganis, C. (2020). Inferring entire spiking activity from local field potentials. <em>Scientific reports.</em> 11. <a href=""https://doi.org/10.1038/s41598-021-98021-9"">doi:10.1038/s41598-021-98021-9</a></li> <li>Sachdeva, P. S,&nbsp;Livezey, J. A,&nbsp;Dougherty, M. E.,&nbsp;Gu, B.-M., Berke, J. D, &amp; Bouchard, K. E. (2020). Accurate Inference in Parametric Models Reshapes Neuroscientific Interpretation and Improves Data-driven Discovery. <em>bioRxiv Preprint.</em>&nbsp;2020.04.10.036244.&nbsp;<a href=""https://doi.org/10.1101/2020.04.10.036244"">doi:10.1101/2020.04.10.036244</a></li> <li>Pei, F., Ye, J., Zoltowski, D., Wu, A., Chowdhury, R. H., Sohn, H., O'Doherty, J. E., Shenoy, K. V., Kaufman, M. T., Churchland, M., Jazayeri, M., Miller, L. E., Pillow, J., Park, I. M., Dyer, E. L., &amp; Pandarinath, C. (2021). Neural Latents Benchmark '21: Evaluating latent variable models of neural population activity. <em>arXiv Preprint.</em>&nbsp;<a href=""http://arxiv.org/abs/2109.04463"">arXiv:2109.04463</a></li> <li>Jensen, K. T., Kao, T.-C., Stone, J. T., &amp; Hennequin, G. (2021). Scalable Bayesian GPFA with automatic relevance determination and discrete noise models. <em>bioRxiv Preprint.</em> 2021.06.03.446788.&nbsp;<a href=""https://doi.org/10.1101/2021.06.03.446788"">doi:10.1101/2021.06.03.44678</a></li> <li>Savolainen, O.W. (2021). The significance of neural inter-frequency power correlations.&nbsp;<em>Sci. Rep.</em>&nbsp;11,&nbsp;23190.&nbsp;<a href=""https://doi.org/10.1038/s41598-021-02277-0"">doi:10.1038/s41598-021-02277-0</a></li> <li>Schimel, M., Kao, T.-C., Jensen, K.T., &amp; Hennequin, G. (2021). iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data. <em>bioRxiv Preprint.</em> 2021.10.07.463540.&nbsp;<a href=""https://doi.org/10.1101/2021.10.07.463540"">doi:10.1101/2021.10.07.463540</a></li> <li>Li, Y., Qi, Y., Wang,&nbsp;Y., Wang, Y., Xu, K., &amp; Pan, G. (2021). Robust neural decoding by kernel regression with Siamese representation learning. <em>J Neural Eng.&nbsp;</em>18(5):&nbsp;056062. <a href=""http://doi.org/10.1088/1741-2552/ac2c4e"">doi:10.1088/1741-2552/ac2c4e</a></li> <li>Savolainen, O. W.&nbsp;(2021). The Significance of Neural Inter-Frequency Correlations. <em>Research Square Preprint&nbsp;(v1)</em>. <a href=""https://doi.org/10.21203/rs.3.rs-329644/v1"">doi:10.21203/rs.3.rs-329644/v1</a></li> <li>Sani, O. G., Pesaran, B., &amp; Shanechi., M. M. (2021). Where is all the nonlinearity: flexible nonlinear modeling of behaviorally relevant neural dynamics using recurrent neural networks. <em>bioRxiv Preprint.</em> 2021.09.03.458628.&nbsp;<a href=""https://doi.org/10.1101/2021.09.03.458628"">doi:10.1101/2021.09.03.458628</a></li> <li>Yang, S.-H., Huang, J.-W., Huang, C.-J., Chiu, P.-H., Lai, H.-Y., &amp; Chen, Y.-Y. (2021). Selection of Essential Neural Activity Timesteps for Intracortical Brain&ndash;Computer Interface Based on Recurrent Neural Network.&nbsp;<em>Sensors</em>. 21(19): 6372. <a href=""https://doi.org/10.3390/s21196372"">doi:10.3390/s21196372</a></li> <li>Sachdeva, P. S.,&nbsp;Livezey, J. A.,&nbsp;Dougherty, M. E.,&nbsp;Gu, B.-M., Berke, J. D., &amp; Bouchard, K. E. (2021). Improved inference in coupling, encoding, and decoding models and its consequence for neuroscientific interpretation.&nbsp;<em>Journal of Neuroscience Methods. </em>358:&nbsp;109195.&nbsp;<a href=""http://doi.org/10.1016/j.jneumeth.2021.109195"">doi:10.1016/j.jneumeth.2021.109195</a></li> <li>Ahmadi, N.,&nbsp;Constandinou, T. G., &amp; Bouganis. C.-S. (2021).&nbsp;Impact of referencing scheme on decoding performance of LFP-based brain-machine interface.&nbsp;<em>J Neural Eng. </em>18(1):&nbsp;016028.&nbsp;<a href=""https://doi.org/10.1088/1741-2552/abce3c"">doi:10.1088/1741-2552/abce3c</a></li> <li>Ahmadi, N.,&nbsp;Constandinou, T.&nbsp;G., &amp; Bouganis, C.-S.&nbsp;(2021).&nbsp;Robust and accurate decoding of hand kinematics from entire spiking activity using deep learning. <em>J. Neural Eng.</em>&nbsp;18(2): 026011.&nbsp;<a href=""https://doi.org/10.1088/1741-2552/abde8a"">doi:10.1088/1741-2552/abde8a</a></li> <li>Keshtkaran,&nbsp;M. R.,&nbsp;Sedler, A. R., Chowdhury,&nbsp;R.&nbsp;H.,&nbsp;Tandon, R.,&nbsp;Basrai,&nbsp;D.,&nbsp;Nguyen, S. L, Sohn,&nbsp;H.,&nbsp;Jazayeri,&nbsp;M.,&nbsp;Miller, L. E., &amp; Pandarinath, C.&nbsp;(2021). A large-scale neural network training framework for generalized estimation of single-trial population dynamics. <em>bioRxiv Preprint</em>. 2021.01.13.426570.&nbsp;<a href=""https://doi.org/10.1101/2021.01.13.426570"">doi:10.1101/2021.01.13.426570</a></li> <li>Qi, Y., Zhu, X., Xu, K., Ren, F., Jiang, H., Zhu, J., Zhang, J., Pan, G., &amp; Wang, Y. (2022).&nbsp;Dynamic Ensemble Bayesian Filter for Robust Control of a Human Brain-Machine Interface.&nbsp;<em>IEEE Transactions on Biomedical Engineering.</em> 69(12): 3825-3835.&nbsp;<a href=""https://doi.org/10.1109/TBME.2022.3182588"">doi:10.1109/TBME.2022.3182588</a></li> <li>Keshtkaran, M. R., Sedler, A. R., Chowdhury, R. H., Tandon, R., Basrai, D., Nguyen, S. L., Sohn, H., Jazayeri, M., Miller, L. E., &amp; Pandarinath, C. (2022). A large-scale neural network training framework for generalized estimation of single-trial population dynamics.&nbsp;<em>Nat Methods.</em>&nbsp;19, 1572-1577. <a href=""https://doi.org/10.1038/s41592-022-01675-0"">doi:10.1038/s41592-022-01675-0</a></li> <li>Savolainen, O. W. (2022). Hardware-efficient data compression in wireless intracortical brain-machine interfaces. <em>PhD Dissertation</em>.&nbsp;<a href=""https://doi.org/10.25560/105363"">doi:10.25560/105363</a></li> <li>Savolainen, O. W., Zhang, Z., Feng, P. &amp; Constandinou, T. G. (2022). Hardware-Efficient Compression of Neural Multi-Unit Activity. <em>bioRxiv Preprint</em>. 2022.03.25.485863 <a href=""https://doi.org/10.1101/2022.03.25.485863"">doi:10.1101/2022.03.25.485863</a></li> <li>Savolainen, O. W., Zhang, Z. &amp; Constandinou, T. G. (2022). Ultra low power, event-driven data compression of Multi-Unit activity. <em>bioRxiv Preprint</em>. 2022.11.24.517853 <a href=""https://doi.org/10.1101/2022.11.24.517853"">doi:10.1101/2022.11.24.517853</a></li> <li>Meng, R., Luo, T. &amp; Bouchard, K. (2022). Compressed Predictive Information Coding. <em>arXiv Preprint.</em> <a href=""https://arxiv.org/abs/2203.02051"">arXiv:2203.02051</a></li> <li>Li, Y., Zhu, X., Qi, Y. &amp; Wang, Y. (2022). Revealing unexpected complex encoding but simple decoding mechanisms in motor cortex via separating behaviorally relevant neural signals. <em>bioRxiv Preprint</em>. <a href=""https://doi.org/10.1101/2022.11.13.515644"">doi:10.1101/2022.11.13.515644</a></li> <li>Ahmadi, N., Adiono, T., Purwarianti, A., Constandinou, T. G. &amp; Bouganis, C.-S. (2022). Improved spike-based brain-machine interface using Bayesian adaptive kernel smoother and deep learning. <em>IEEE Access.</em> 10: 29341-29356. <a href=""https://doi.org/10.1109/access.2022.3159225"">doi:10.1109/access.2022.3159225</a></li> <li>Zhu, X., Qi, Y., Pan, G., Wang, Y. (2022). <a href=""https://papers.neurips.cc/paper_files/paper/2022/hash/8dcc306a2522c60a78f047ab8739e631-Abstract-Conference.html"">Tracking Functional Changes in Nonstationary Signals with Evolutionary Ensemble Bayesian Model for Robust Neural Decoding</a>.&nbsp;<em>Advances in Neural Information Processing Systems (NeurIPS)&nbsp;35.</em></li> <li>Savolainen, O. W., Zhang, Z., Feng, P. &amp; Constandinou, T. G. (2022). Hardware-Efficient Compression of Neural Multi-Unit Activity. <em>IEEE Access</em>. 10: 117515-117529. <a href=""https://doi.org/10.1109/access.2022.3219441"">doi:10.1109/access.2022.3219441</a></li> <li>Qi, Y., Zhu, X., Xu, K., Ren, F., Jiang, H., Zhu, J., Zhang, J., Pan, G., &amp; Wang, Y. (2022).&nbsp;Dynamic Ensemble Bayesian Filter for Robust Control of a Human Brain-Machine Interface. <em>arXiv Preprint.&nbsp;</em><a href=""https://arxiv.org/abs/2204.11840""><em>arXiv:2204.11840</em></a></li> <li>Valencia, D., Mercier, P. P, &amp; Alimohammad, A. (2022).&nbsp;<em>In vivo</em>&nbsp;neural spike detection with adaptive noise estimation. <em>J Neural Eng.</em> 19: 046018.&nbsp;<a href=""https://doi.org/10.1088/1741-2552/ac8077"">doi:10.1088/1741-2552/ac8077</a></li> <li>Zhang, Z., Feng, P., Oprea, A. &amp; Constandinou, T. G. (2023). Calibration-free and hardware-efficient neural spike detection for brain machine interfaces. <em>IEEE transactions on biomedical circuits and systems.</em> 17(4): 725-740. <a href=""https://doi.org/10.1109/TBCAS.2023.3278531"">doi:10.1109/TBCAS.2023.3278531</a></li> <li>Biyan, Z., Sun, P. &amp; Basu, A. (2023). Combining SNNs with filtering for efficient neural decoding in implantable brain-machine interfaces. <em>Neuromorphic Computing and Engineering.</em> 5. <a href=""https://doi.org/10.1088/2634-4386/adba82"">doi:10.1088/2634-4386/adba82</a></li> <li>Zhang, Z. (2023). Real-time neural signal processing and low-power hardware co-design for wireless implantable brain machine interfaces. <em>PhD Dissertation. </em><a href=""https://doi.org/10.25560/108113"">doi:10.25560/108113</a></li> <li>Zhou, B., Sun, P. V. &amp; Basu, A. (2023). Combining SNNs with filtering for efficient neural decoding in implantable brain-machine interfaces. <em>arXiv Preprint</em>. arXiv:XXXX</li> <li>Song, C. Y. &amp; Shanechi, M. M. (2023). Unsupervised learning of stationary and switching dynamical system models from Poisson observations. <em>Journal of neural engineering.</em> 20(6). doi:10.1088/1741-2552/ad038d</li> <li>Bono, M. (2023). Time robustness of deep learning models for real-time neural decoding of arm movement. <em>PhD Dissertation.</em> doi:XXXX</li> <li>Azabou, M., Arora, V., Ganesh, V., Mao, X., Nachimuthu, S., Mendelson, M. J., Richards, B., Perich, M. G., Lajoie, G. &amp; Dyer, E. L. (2023). A unified, scalable framework for neural population decoding. <em>arXiv Preprint. </em>arXiv:XXXX</li> <li>Yik, J., Berghe, K., Blanken, D. d., Bouhadjar, Y., Fabre, M., Hueber, P., Ke, W., Khoei, M. A., Kleyko, D., Pacik-Nelson, N., Pierro, A., Stratmann, P., Sun, P. V., Tang, G., Wang, S., Zhou, B., Ahmed, S. H., Joseph, G. V., Leto, B., Micheli, A., Mishra, A. K., Lenz, G., Sun, T., Ahmed, Z., Akl, M., Anderson, B., Andreou, A. G., Bartolozzi, C., Basu, A., Bogdan, P., Bohte, S., Buckley, S., Cauwenberghs, G., Chicca, E., Corradi, F., Croon, G., Danielescu, A., Daram, A., Davies, M., Demirag, Y., Eshraghian, J., Fischer, T., Forest, J., Fra, V., Furber, S., Furlong, P. M., Gilpin, W., Gilra, A., Gonzalez, H. A., Indiveri, G., Joshi, S., Karia, V., Khacef, L., Knight, J. C., Kriener, L., Kubendran, R., Kudithipudi, D., Liu, S., Liu, Y., Ma, H., Manohar, R., Margarit-Taul&eacute;, J. M., Mayr, C., Michmizos, K., Muir, D. R., Neftci, E., Nowotny, T., Ottati, F., Ozcelikkale, A., Panda, P., Park, J., Payvand, M., Pehle, C., Petrovici, M. A., Posch, C., Renner, A., Sandamirskaya, Y., Schaefer, C. J. S., Schaik, A., Schemmel, J., Schmidgall, S., Schuman, C., Seo, J., Sheik, S., Shrestha, S. B., Sifalakis, M., Sironi, A., Stewart, K., Stewart, M., Stewart, T. C., Timcheck, J., T&ouml;men, N., Urgese, G., Verhelst, M., Vineyard, C. M., Vogginger, B., Yousefzadeh, A., Zohora, F. T., Frenkel, C. &amp; Reddi, V. J. (2023). NeuroBench: A framework for benchmarking neuromorphic computing algorithms and systems. <em>arXiv Preprint.</em> arXiv:XXXX</li> <li>Zhang, Z. &amp; Constandinou, T. G. (2023). Firing-rate-modulated spike detection and neural decoding co-design. <em>Journal of neural engineering.</em> 20(3). doi:10.1088/1741-2552/accece</li> <li>Ye, J., Collinger, J. L., Wehbe, L., &amp; Gaunt, R. (2023). Neural Data Transformer 2: Multi-Context Pretraining for Neural Spiking Activity. <em>bioRxiv Preprint</em>. 2023.09.18.558113. <a href=""https://doi.org/10.1101/2023.09.18.558113"">doi:10.1101/2023.09.18.558113</a></li> <li>Abbaspourazad, H., Erturk, E., Pesaran, B. &amp; Shanechi, M. (2023). Dynamical flexible inference of nonlinear latent structures in neural population activity. bioRxiv Preprint. doi:XXXX</li> <li>Asahina, T., Shimba, K., Kotani, K. &amp; Jimbo, Y. (2023). Improving the accuracy of decoding monkey brain-machine interface data by estimating the state of unobserved cell assemblies. J<em>ournal of neuroscience methods.</em> 385(109764): 109764. doi:10.1016/j.jneumeth.2022.109764</li> <li>Meghanath, G., Jimenez, B. &amp; Makin, J. G. (2023). Inferring population dynamics in macaque cortex. <em>Journal of neural engineering. </em>20(5). doi:10.1088/1741-2552/ad0651</li> <li>Abbaspourazad, H., Erturk, E., Pesaran, B. &amp; Shanechi, M. M. (2024). Dynamical flexible inference of nonlinear latent factors and structures in neural population activity. <em>Nature biomedical engineering. </em>8(1): 85-108. doi:10.1038/s41551-023-01106-1</li> <li>Valencia, D. (2024). Towards Autonomous Brain-Computer Interfaces: Approaches, Design, and Implementation. <em>PhD Dissertation.</em> doi:XXXX</li> <li>Vasilache, A., Krausse, J., Knobloch, K. &amp; Becker, J. (2024). Hybrid spiking neural networks for low-power intra-cortical brain-machine interfaces. <em>arXiv Preprint.</em> arXiv:XXXX</li> <li>Weng, Y., Qi, Y., Wang, Y. &amp; Pan, G. (2024). Neuromorphic model-based neural decoders for brain-computer interfaces: a comparative study. doi:10.1109/biocas61083.2024.10798332</li> <li>Martis, L., Leone, G., Raffo, L. &amp; Meloni, P. (2024). Low-power FPGA-based spiking neural networks for real-time decoding of intracortical neural activity. <em>IEEE sensors journal.</em> 24(24): 42448-42459. doi:10.1109/jsen.2024.3487021</li> <li>Oganesian, L. L., Sani, O. G. &amp; Shanechi, M. (2024). Spectral learning of shared dynamics between generalized-linear processes. <em>Neural Information Processing Systems. </em>37: 89150-89183.</li> <li>Tasca, M. (2024). Time-Robust and Energy-Efficient Decoder for Real-Time Neural Decoding of Primary Motor Cortex Activity. <em>PhD Dissertation.</em> doi:XXXX</li> <li>Wang, Y., Wang, Z. &amp; Liu, S. (2024). Leveraging recurrent neural networks for predicting motor movements from primate motor cortex neural recordings. <em>arXiv Preprint. </em>arXiv:XXXX</li> <li>Liu, T., Gygax, J., Rossbroich, J., Chua, Y., Zhang, S. &amp; Zenke, F. (2024). Decoding finger velocity from cortical spike trains with recurrent spiking neural networks. <em>arXiv Preprint. </em>arXiv:XXX</li> <li>Schulz, A., Vetter, J., Gao, R., Morales, D., Lobato-Rios, V., Ramdya, P., Gon&ccedil;alves, P. J. &amp; Macke, J. H. (2024). Modeling conditional distributions of neural and behavioral data with masked variational autoencoders. <em>bioRxiv Preprint. </em>doi:10.1101/2024.04.19.590082</li> <li>Sani, O. G., Pesaran, B. &amp; Shanechi, M. M. (2024). Dissociative and prioritized modeling of behaviorally relevant neural dynamics using recurrent neural networks. <em>Nature Neuroscience. </em>27(10): 2033-2045. doi:10.1038/s41593-024-01731-2</li> <li>Kumar, A., Frank, L. M. &amp; Bouchard, K. E. (2024). Identifying feedforward and feedback controllable subspaces of neural population dynamics. <em>arXiv Preprint.</em> arXiv:XXXX</li> <li>McCart, J. D., Sedler, A. R., Versteeg, C., Mifsud, D., Rigotti-Thompson, M. &amp; Pandarinath, C. (2024). Diffusion-based generation of neural activity from disentangled latent Codes. <em>arXiv Preprint. </em>arXiv:XXXX</li> <li>Bouchard, K. &amp; Kumar, A. (2024). Feedback controllability is a normative theory of neural population dynamics. <em>Research Square. </em>doi:10.21203/rs.3.rs-4102129/v1</li> <li>Yang, S., Huang, C. &amp; Huang, J. (2024). Increasing robustness of intracortical brain-computer interfaces for recording condition changes via data augmentation. <em>Computer methods and programs in biomedicine. </em>251(108208): 108208. doi:10.1016/j.cmpb.2024.108208</li> <li>Wang, C., Yin, M., Liang, F. &amp; Wang, X. (2024). A robust and high accurate method for hand kinematics decoding from neural populations. doi:10.1007/978-981-99-8546-3\_20</li> <li>Mohan, V., Tay, W. P. &amp; Basu, A. (2025). Towards neuromorphic compression based neural sensing for next-generation wireless implantable brain machine interface. <em>Neuromorphic Computing and Engineering.</em> 5(1): 014004. doi:10.1088/2634-4386/adad10</li> <li>Vahidi, P., Sani, O. G. &amp; Shanechi, M. (2025). BRAID: Input-driven nonlinear dynamical modeling of neural-behavioral data. International Conference on Learning Representations.</li> <li>Leone, G., Martis, L., Raffo, L. &amp; Meloni, P. (2025). Enabling SNN-based near-MEA neural decoding with channel selection: An open-HW approach. doi:10.23919/date64628.2025.10993220</li> <li>Mohan, V., Zhou, B., Wang, Z., Bharath, A., Drakakis, E. &amp; Basu, A. (2025). Architectural exploration of hybrid neural decoders for neuromorphic implantable BMI. <em>arXiv Preprint. </em>arXiv:XXXX</li> <li>Yik, J., Berghe, K., Blanken, D., Bouhadjar, Y., Fabre, M., Hueber, P., Ke, W., Khoei, M. A., Kleyko, D., Pacik-Nelson, N., Pierro, A., Stratmann, P., Sun, P. V., Tang, G., Wang, S., Zhou, B., Ahmed, S. H., Vathakkattil Joseph, G., Leto, B., Micheli, A., Mishra, A. K., Lenz, G., Sun, T., Ahmed, Z., Akl, M., Anderson, B., Andreou, A. G., Bartolozzi, C., Basu, A., Bogdan, P., Bohte, S., Buckley, S., Cauwenberghs, G., Chicca, E., Corradi, F., Croon, G., Danielescu, A., Daram, A., Davies, M., Demirag, Y., Eshraghian, J., Fischer, T., Forest, J., Fra, V., Furber, S., Furlong, P. M., Gilpin, W., Gilra, A., Gonzalez, H. A., Indiveri, G., Joshi, S., Karia, V., Khacef, L., Knight, J. C., Kriener, L., Kubendran, R., Kudithipudi, D., Liu, S., Liu, Y., Ma, H., Manohar, R., Margarit-Taul&eacute;, J. M., Mayr, C., Michmizos, K., Muir, D. R., Neftci, E., Nowotny, T., Ottati, F., Ozcelikkale, A., Panda, P., Park, J., Payvand, M., Pehle, C., Petrovici, M. A., Posch, C., Renner, A., Sandamirskaya, Y., Schaefer, C. J. S., Schaik, A., Schemmel, J., Schmidgall, S., Schuman, C., Seo, J., Sheik, S., Shrestha, S. B., Sifalakis, M., Sironi, A., Stewart, K., Stewart, M., Stewart, T. C., Timcheck, J., T&ouml;men, N., Urgese, G., Verhelst, M., Vineyard, C. M., Vogginger, B., Yousefzadeh, A., Zohora, F. T., Frenkel, C. &amp; Reddi, V. J. (2025). The neurobench framework for benchmarking neuromorphic computing algorithms and systems. <em>Nature Communications. </em>16(1): 1545. doi:10.1038/s41467-025-56739-4</li> <li>Zheng, J., Li, Y., Chen, L., Wang, F., Gu, B., Sun, Q., Gao, X. &amp; Zhou, F. (2025). Effects of packet loss on neural decoding effectiveness in wireless transmission. <em>Brain Sciences.</em> 15(3): 221. doi:10.3390/brainsci15030221</li> </ol> <p><strong>History.</strong></p> <ul> <li>Version 2 - added CSV of results from Makin et al.</li> <li>Version 1 - initial release.</li> </ul>",The science and engineering behind sensitized brain-controlled bionic hands,https://doi.org/10.1152/physrev.00034.2020,69,
Dataset for Automatic Region-based Coronary Artery Disease Diagnostics Using X-Ray Angiography Images,10390295,"<p>ARCADE: Automatic Region-based Coronary Artery Disease diagnostics using x-ray angiography imagEs Dataset Phase 2 consist of two folders with 300 images in each of them as well as annotations.&nbsp;</p> <p>ARCADE: Automatic Region-based Coronary Artery Disease diagnostics using x-ray angiography imagEs Dataset Phase 1 consists of two datasets of XCA images for each of two tasks of ARCADE challenge.&nbsp;The first task includes in total 1200 coronary vessel tree images, which are divided into train(1000) and validation(200) groups, images for training are followed with annotations,&nbsp;depicting the division of a heart into 26 different regions based on the Syntax Score methodology[1]. Similarly, the second task includes a different set of 1200 images with same train-val division proportion&nbsp;with annotated regions containing atherosclerotic plaques. This dataset, carefully annotated by medical experts, enables scientists to actively contribute towards the advancement of an automated risk assessment system for patients with CAD.&nbsp;</p> <p>The dataset structure is as follows: top-level directories ""syntax"" and ""stenosis"" contain files for the two dataset objectives, namely: i) vessel branch classification according to the SYNTAX methodology; and ii) stenosis detection. Inside both directories, there are 3 subsets of the dataset, such as ""train"", ""val"", and ""test"". Inside each of those folders, there are 2 lower-level directories - ""images"", and ""annotations"". Inside the ""images"" folder there are images in "".png"" format, extracted from DICOM recordings. The ""annotations"" folders contain single "".JSON"" files, which are named in correspondence to the objective, i.e. ""train.JSON"", ""val.JSON"", and ""test.JSON"".</p> <p>The structure of "".JSON"" contains three top-level fields: ""images"", ""categories"", and ""annotations"". The ""images"" field contains the unique ""id"" of the image in the dataset, its ""width"" and ""height"" in pixels, and the ""file_name"" sub-field, which contains specific information about the image. The ""categories"" field contains a unique ""id"" from 1 to 26, and a ""name"", relating it to the SYNTAX descriptions. The ""annotations"" field contains a unique ""id"" of the annotation, ""image_id"" value, relating it to the specific image from the ""images"" field, and a ""category_id"" relating it to the specific category from the ""categories"" field. The ""segmentation"" sub-field contains coordinates of mask edge points in ""XYXY"" format. Bounding box coordinates are given in the ""bbox"" field in the ""XYWH"" format, where the first 2 values represent the x and y coordinates of the left-most and top-most points in the segmentation mask. The height and width of the bounding box are determined by the difference between the right-most and bottom-most points and the first two values. Finally, the ""area"" field provides the total area of the bounding box, calculated as the area of a rectangle.</p> <p>&nbsp;</p> <p>The corresponding Dataset Article will be provided later.&nbsp;</p> <p>[1]&nbsp;Syntax score segment definitions. https://syntaxscore.org/index.php/tutorial/definitions/14-appendix-i-segment-definitions</p>",Dataset for Automatic Region-based Coronary Artery Disease Diagnostics Using X-Ray Angiography Images,https://doi.org/10.1038/s41597-023-02871-z,52,pdfs/Dataset_for_Automatic_Region-based_Coronary_Artery_Disease_Diagnostics_Using_X-R.pdf
MicrobiomeHD: the human gut microbiome in health and disease,1146764,"<p><strong>Overview</strong></p>  <p>MicrobiomeHD is a standardized database of human gut microbiome studies in health and disease. This database includes publicly available 16S data from published case-control studies and their associated patient metadata. Raw sequencing data for each study was downloaded and processed through a standardized pipeline.</p>  <p>To be included in MicrobiomeHD, datasets have:</p>  <ul> 	<li>publicly available raw sequencing data (fastq or fasta)</li> 	<li>publicly available metadata with at least case and control labels for each patient</li> </ul>  <p>Currently, MicrobiomeHD is focused on stool samples. Additional samples may be included in certain datasets, as indicated in the metadata.</p>  <p><strong>Files</strong></p>  <p>Additional information about the datasets included in this MicrobiomeHD release are in the MicrobiomeHD github repo <a href=""https://github.com/cduvallet/microbiomeHD"">https://github.com/cduvallet/microbiomeHD</a>, in the file <em>db/dataset_info.yaml</em>. Top-level identifiers correspond to dataset IDs labeled by disease_first-author. For the most part, sample sizes in the yaml file are those that were described in the papers, and may not exactly reflect the actual data (due to missing/extra data, samples which didn&#39;t pass quality control, etc).</p>  <p>Each dataset was downloaded and processed through a standardized pipeline. The raw processing results are available in the *.tar.gz files here. Each file has the same directory structure and files, as described in the pipeline documentation: <a href=""http://amplicon-sequencing-pipeline.readthedocs.io/en/latest/output.html"">http://amplicon-sequencing-pipeline.readthedocs.io/en/latest/output.html</a>.</p>  <p>Specific files of interest in each *.tar.gz folder include:</p>  <ul> 	<li><strong>summary_file.txt</strong>: this file contains a summary of all parameters used to process the data</li> 	<li><strong>datasetID.metadata.txt</strong>: the metadata associated with the samples. Note that some samples in the metadata may not have sequencing data, and vice versa.</li> 	<li><strong>RDP/datasetID.otu_table.100.denovo.rdp_assigned</strong>: the 100% OTU tables with Latin taxonomic names assigned using the RDP classifier (c = 0.5).</li> 	<li><strong>datasetID.otu_seqs.100.fasta</strong>: representative sequences for each OTU in the 100% OTU table. OTU labels in the OTU table end with <code>d__denovoID</code> - these denovoIDs correspond to the sequences in this file.</li> 	<li><strong>README.txt</strong>: additional information about steps taken to download and process each dataset, as needed.</li> </ul>  <p>The raw data was acquired as described in the supplementary materials of Duvallet et al.&#39;s &quot;Meta analysis of microbiome studies identifies shared and disease-specific patterns&quot; and, when available, the respective dataset README files.</p>  <p>Raw sequencing data was processed with the Alm lab&#39;s in-house 16S processing pipeline: <a href=""https://github.com/thomasgurry/amplicon_sequencing_pipeline"">https://github.com/thomasgurry/amplicon_sequencing_pipeline</a></p>  <p>Pipeline documentation is available at: <a href=""http://amplicon-sequencing-pipeline.readthedocs.io/"">http://amplicon-sequencing-pipeline.readthedocs.io/</a></p>  <p>Metadata was extracted from the original papers and/or data sources, and formatted manually. When possible, these steps are documented in each dataset&#39;s associated README.txt file.</p>  <p><strong>Contributing</strong></p>  <p>MicrobiomeHD is a resource that can be used to extract disease-specific microbiome signals in individual case-control studies. Many microbes respond non-specifically to health and disease, and the majority of bacterial associations within individual studies overlap with this non-specific response. Researchers should cross-check their results with the data presented here to ensure that their identified microbial associations are specific to their disease under study.</p>  <p>We provide an updated list of non-specific microbes here, as well as the raw OTU tables for anyone who wishes to reproduce and adapt this analysis to their study question.</p>  <p>If you would like to include your case-control dataset in MicrobiomeHD, please email ejalm[at]mit.edu and duvallet[at]mit.edu.</p>  <p>For us to process your data through our standard pipeline, you will need to provide the following files and information about your data:</p>  <ul> 	<li>raw sequencing data in fastq or fasta format (preferably fastq)</li> 	<li>information about which processing steps will be required (e.g. removing primers or barcodes, merging paired-end reads, etc)</li> 	<li>sample IDs associated with the sequencing data (either mapped to barcodes still in the sequences, or to each de-multiplexed sequencing file)</li> 	<li>case/control metadata of each sample</li> 	<li>other relevant metadata (e.g. sampling site, if not all samples are stool; sampling time point, if multiple samples per patient were taken; etc)</li> </ul>  <p>By using MicrobiomeHD in your own analyses, you agree to contribute your dataset to this database and to make your raw sequencing data (i.e. fastq files) publicly available.</p>  <p><strong>Citing MicrobiomeHD</strong></p>  <p>The MicrobiomeHD database and original publications for each of these datasets are described in Duvallet et al. (2017): <a href=""http://dx.doi.org/10.1038/s41467-017-01973-8"">http://dx.doi.org/10.1038/s41467-017-01973-8</a></p>  <p>Duvallet, C., Gibbons, S. M., Gurry, T., Irizarry, R. A., &amp; Alm, E. J. (2017). Meta-analysis of gut microbiome studies identifies disease-specific and shared responses. <em>Nature communications</em>, 8(1), 1784.</p>  <p>If you use any of these datasets in your analysis, please cite both MicrobiomeHD (Duvallet et al. (2017)) and the original publication for each dataset that you use.</p>  <p>The code used to process and analyze this data in the paper is available on github: <a href=""https://github.com/cduvallet/microbiomeHD"">https://github.com/cduvallet/microbiomeHD</a></p>  <p><strong>Files</strong></p>  <p><em>Data files</em></p>  <p><strong>file-S3.nonspecific_genera.txt</strong>: Supplemental Table 3 from Duvallet et al. (2017), listing the non-specific health- and disease-associated microbes.<br> <strong>dataset_info.yaml</strong>: yaml file with additional dataset metadata.</p>  <p><em>Datasets</em></p>  <p>Note that MicrobiomeHD contains all 28 datasets from Duvallet et al. (2017), as well as additional datasets which did not meet the inclusion criteria for the meta-analysis presented in the paper. Additional information about the datasets included in this MicrobiomeHD release are in the original publications and the MicrobiomeHD github repo https://github.com/cduvallet/microbiomeHD, and in the file <em>dataset_info.yaml</em>.</p>  <p>The sample sizes listed here reflect what was reported in the original publications. Some may have discrepancies between what is reported and what is in the actual data due to missing data, quality issues, barcode mismatches, etc.</p>  <ul> 	<li><strong>asd_son_results.tar.gz</strong> (<em>asd_son</em>): NT: 44, ASD: 59  	<ul> 		<li>http://dx.doi.org/10.1371/journal.pone.0137725</li> 	</ul> 	</li> 	<li><strong>autism_kb_results.tar.gz</strong> (<em>asd_kang</em>): H: 20, ASD: 20 	<ul> 		<li>http://dx.doi.org/10.1371/journal.pone.0068322</li> 	</ul> 	</li> 	<li><strong>cdi_schubert_results.tar.gz</strong> (<em>cdi_schubert</em>): H: 155, nonCDI: 89, CDI: 94 	<ul> 		<li>http://dx.doi.org/10.1128/mBio.01021-14</li> 	</ul> 	</li> 	<li><strong>cdi_vincent_v3v5_results.tar.gz</strong> (<em>cdi_vincent</em>): H: 25, CDI: 25 	<ul> 		<li>http://dx.doi.org/10.1186/2049-2618-1-18</li> 	</ul> 	</li> 	<li><strong>cdi_youngster_results.tar.gz</strong> (<em>cdi_youngster</em>): H: 4, CDI: 19 	<ul> 		<li>http://dx.doi.org/10.1093/cid/ciu135</li> 	</ul> 	</li> 	<li><strong>crc_baxter_results.tar.gz</strong> (<em>crc_baxter</em>): adenoma: 198, H: 172, CRC: 120 	<ul> 		<li>http://dx.doi.org/10.1186/s13073-016-0290-3</li> 	</ul> 	</li> 	<li><strong>crc_xiang_results.tar.gz</strong> (<em>crc_chen</em>): H: 22, CRC: 21 	<ul> 		<li>http://dx.doi.org/10.1371/journal.pone.0039743</li> 	</ul> 	</li> 	<li><strong>crc_zackular_results.tar.gz</strong> (<em>crc_zackular</em>): adenoma: 30, H: 30, CRC: 30 	<ul> 		<li>http://dx.doi.org/10.1158/1940-6207.CAPR-14-0129</li> 	</ul> 	</li> 	<li><strong>crc_zeller_results.tar.gz</strong> (<em>crc_zeller</em>): H: 75, CRC: 41 	<ul> 		<li>http://dx.doi.org/10.15252/msb.20145645</li> 	</ul> 	</li> 	<li><strong>crc_zhao_results.tar.gz</strong> (<em>crc_wang</em>): H: 56, CRC: 46 	<ul> 		<li>http://dx.doi.org/10.1038/ismej.2011.109}</li> 	</ul> 	</li> 	<li><strong>edd_singh_results.tar.gz</strong> (<em>edd_singh</em>): STEC: 28, CAMP: 71, SALM: 66, SHIG: 34, H: 75 	<ul> 		<li>http://dx.doi.org/10.1186/s40168-015-0109-2</li> 	</ul> 	</li> 	<li><strong>hiv_dinh_results.tar.gz</strong> (<em>hiv_dinh</em>): H: 16, HIV: 21 	<ul> 		<li>http://dx.doi.org/10.1093/infdis/jiu409</li> 	</ul> 	</li> 	<li><strong>hiv_lozupone_results.tar.gz</strong> (<em>hiv_lozupone</em>): H: 13, HIV: 25 	<ul> 		<li>http://dx.doi.org/10.1016/j.chom.2013.08.006</li> 	</ul> 	</li> 	<li><strong>hiv_noguerajulian_results.tar.gz</strong> (<em>hiv_noguerajulian</em>): H: 34, HIV: 206 	<ul> 		<li>https://doi.org/10.1016%2Fj.ebiom.2016.01.032</li> 	</ul> 	</li> 	<li><strong>ibd_alm_results.tar.gz</strong> (<em>ibd_papa</em>): IBDundef: 1, nonIBD: 24, UC: 43, CD: 23 	<ul> 		<li>http://dx.doi.org/10.1371/journal.pone.0039242</li> 	</ul> 	</li> 	<li><strong>ibd_engstrand_maxee_results.tar.gz</strong> (<em>ibd_willing</em>): CCD: 12, H: 35, ICD: 15, UC: 16, ICCD: 2 	<ul> 		<li>http://dx.doi.org/10.1053/j.gastro.2010.08.049</li> 	</ul> 	</li> 	<li><strong>ibd_gevers_2014_results.tar.gz</strong> (<em>ibd_gevers</em>): H: 31, CD: 224 	<ul> 		<li>http://dx.doi.org/10.1016/j.chom.2014.02.005</li> 	</ul> 	</li> 	<li><strong>ibd_huttenhower_results.tar.gz</strong> (<em>ibd_morgan</em>): H: 18, UC: 48, CD: 62 	<ul> 		<li>http://dx.doi.org/10.1186/gb-2012-13-9-r79</li> 	</ul> 	</li> 	<li><strong>mhe_zhang_results.tar.gz</strong> (<em>liv_zhang</em>): CIRR: 25, H: 26, MHE: 26 	<ul> 		<li>http://dx.doi.org/10.1038/ajg.2013.221</li> 	</ul> 	</li> 	<li><strong>nash_chan_results.tar.gz</strong> (<em>nash_wong</em>): H: 22, NASH: 16 	<ul> 		<li>http://dx.doi.org/10.1371/journal.pone.0062885</li> 	</ul> 	</li> 	<li><strong>nash_ob_baker_results.tar.gz</strong> (<em>nash_ob_zhu</em>): H: 16, NASH: 22, OB: 25 	<ul> 		<li>http://dx.doi.org/10.1002/hep.26093</li> 	</ul> 	</li> 	<li><strong>ob_escobar_results.tar.gz</strong> (<em>ob_escobar</em>): OW: 10, H: 10, OB: 10 	<ul> 		<li>https://doi.org/10.1186/s12866-014-0311-6</li> 	</ul> 	</li> 	<li><strong>ob_goodrich_results.tar.gz</strong> (<em>ob_goodrich</em>): OW: 322, H: 433, OB: 183 	<ul> 		<li>http://dx.doi.org/10.1016/j.cell.2014.09.053</li> 	</ul> 	</li> 	<li><strong>ob_gordon_2008_v2_results.tar.gz</strong> (<em>ob_turnbaugh</em>): H: 61, OB: 219 	<ul> 		<li>http://dx.doi.org/10.1038/nature07540</li> 	</ul> 	</li> 	<li><strong>ob_jumpertz_results.tar.gz</strong> (<em>ob_jumpertz</em>): H: 12, OB: 9 	<ul> 		<li>http://ajcn.nutrition.org/content/early/2011/05/03/ajcn.110.010132</li> 	</ul> 	</li> 	<li><strong>ob_ross_results.tar.gz</strong> (<em>ob_ross</em>): H: 26, OB: 37 	<ul> 		<li>http://dx.doi.org/10.1186/s40168-015-0072-y</li> 	</ul> 	</li> 	<li><strong>ob_wu_results.tar.gz</strong> (<em>ob_wu</em>): bmi_data: 101 	<ul> 		<li>http://dx.doi.org/10.1126/science.1208344</li> 	</ul> 	</li> 	<li><strong>ob_zeevi_results.tar.gz</strong> (<em>ob_zeevi</em>): bmi_data: 870 	<ul> 		<li>http://dx.doi.org/10.1016/j.cell.2015.11.001</li> 	</ul> 	</li> 	<li><strong>ob_zupancic_results.tar.gz</strong> (<em>ob_zupancic</em>): H: 167, OB: 117 	<ul> 		<li>http://dx.doi.org/10.1371/journal.pone.0043052</li> 	</ul> 	</li> 	<li><strong>par_scheperjans_results.tar.gz</strong> (<em>par_scheperjans</em>): H: 72, PAR: 72 	<ul> 		<li>http://dx.doi.org/10.1002/mds.26069</li> 	</ul> 	</li> 	<li><strong>ra_littman_results.tar.gz</strong> (<em>art_scher</em>): H: 28, NORA: 44, CRA: 26, PSA: 16 	<ul> 		<li>http://dx.doi.org/10.7554/eLife.01202</li> 	</ul> 	</li> 	<li><strong>t1d_alkanani_results.tar.gz</strong> (<em>t1d_alkanani</em>): T1D: 21, H: 55, T1D_new-onset: 35 	<ul> 		<li>http://dx.doi.org/10.2337/db14-1847</li> 	</ul> 	</li> 	<li><strong>t1d_mejialeon_results.tar.gz</strong> (<em>t1d_mejialeon</em>): T1D: 21, H: 8 	<ul> 		<li>http://dx.doi.org/10.1038/srep03814</li> 	</ul> 	</li> </ul>  <p><strong>Version changes</strong></p>  <p>Version 3</p>  <ul> 	<li>added missing ob_escobar metadata</li> 	<li>added ob_jumpertz, ob_zeevi, and ob_wu</li> 	<li>added README.txt files to all folders, with info about data downloading and processing steps</li> 	<li>removed deprecated quality_control folders from all dataset results</li> 	<li>changed Supplemental File S3 to the most updated version of non-specific genera (as published in Duvallet et al 2017)</li> </ul>  <p>Version 2</p>  <ul> 	<li>added crc_zhu and ob_escobar datasets</li> 	<li>added list of core genera and dataset_info.yaml</li> </ul>",Meta-analysis of gut microbiome studies identifies disease-specific and shared responses,https://doi.org/10.1038/s41467-017-01973-8,964,pdfs/Meta-analysis_of_gut_microbiome_studies_identifies_disease-specific_and_shared_r.pdf
A single-cell and spatially resolved atlas of human breast cancers | spatial transcriptomics data,4739739,"<p>This repository contains spatial transcriptomics data related&nbsp;to the Wu et al. 2021 study&nbsp;&quot;<strong>A single-cell and spatially resolved atlas of human breast cancers</strong>&quot;. Processed count matrices, brightfield HE-images (plain and annotated) and meta-data (containing clinical information and spot pathological details) for&nbsp;6 primary breast cancers profiled using the Visium assay (10X Genomics).&nbsp;If you use this dataset in your research, please consider citing the above study.</p>  <p>The content of the files are:<br> raw_count_matrices.tar.gz - spaceranger processed raw count matrices.</p>  <p>spatial.tar.gz - spaceranger processed spatial files (images, scalefactors, aligned fiducials, position lists)</p>  <p>filtered_count_matrices.tar.gz - filtered count matrices.</p>  <p>metadata.tar.gz - metadata for tissues and spots of filtered count matrices, including clinical subtype and pathological annotation of each spot.</p>  <p>images.pdf&nbsp;- pdf detailing the H&amp;E and annotation images.</p>",Liver tumour immune microenvironment subtypes and neutrophil heterogeneity,https://doi.org/10.1038/s41586-022-05400-x,794,
Summary-level data from meta-analysis of fat distribution phenotypes in UK Biobank and GIANT,1251813,"<p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p>  <p>Summary-level data as presented in:</p>  <p>&quot;Meta-analysis of genome-wide association studies for body fat distribution in 694,649 individuals of European ancestry.&quot; Pulit, SL et al. bioRxiv, 2018. https://www.biorxiv.org/content/early/2018/04/18/304030</p>  <p>**If you use these data, please cite the above preprint.</p>  <p>If you have any questions or comments regarding these files, please contact me:</p>  <p>Sara L Pulit<br> spulit@well.ox.ac.uk or s.l.pulit@umcutrecht.nl</p>  <p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p>  <p><strong>(1) Data files</strong></p>  <p><em>i. whradjbmi.giant-ukbb.meta-analysis.combined.23May2018.txt</em><br> Meta-analysis of waist-to-hip ratio adjusted for body mass index (whradjbmi) in UK Biobank and GIANT data. Combined set of samples, max N = 694,649.</p>  <p><em>ii. whradjbmi.giant-ukbb.meta-analysis.females.23May2018.txt</em><br> Meta-analysis of whradjbmi in UK Biobank and GIANT data. Female samples only, max N = 379,501.</p>  <p><em>iii. whradjbmi.giant-ukbb.meta-analysis.males.23May2018.txt</em><br> Meta-analysis of whradjbmi in UK Biobank and GIANT data. Male samples only, max N = 315,284.</p>  <p><em>iv. whr.giant-ukbb.meta-analysis.combined.23May2018.txt</em><br> Meta-analysis of waist-to-hip ratio (whr) in UK Biobank and GIANT data. Combined set of samples, max N = 697,734.</p>  <p><em>v. whr.giant-ukbb.meta-analysis.females.23May2018.txt</em><br> Meta-analysis of whr in UK Biobank and GIANT data. Female samples only, max N = 381,152.</p>  <p><em>vi. whr.giant-ukbb.meta-analysis.males.23May2018.txt</em><br> Meta-analysis of whr in UK Biobank and GIANT data. Male samples only, max N = 316,772.</p>  <p><em>vii. bmi.giant-ukbb.meta-analysis.combined.23May2018.txt</em><br> Meta-analysis of body mass index (bmi) in UK Biobank and GIANT data. Combined set of samples, max N = 806,834.</p>  <p><em>viii. bmi.giant-ukbb.meta-analysis.females.23May2018.txt</em><br> Meta-analysis of bmi in UK Biobank and GIANT data. Female samples only, max N = 434,794.</p>  <p><em>ix. bmi.giant-ukbb.meta-analysis.males.23May2018.txt</em><br> Meta-analysis of bmi in UK Biobank and GIANT data. Male samples only, max N = 374,756.</p>  <p><strong>(2) Data file format</strong></p>  <p>CHR:&nbsp;Chromosome</p>  <p>POS:&nbsp;Chromosomal position of the SNP, build hg19</p>  <p>SNP: the dbSNP151 identifier of the SNP, followed by the first allele and second allele of the SNP, delimited with a colon. A small number of SNPs (&lt;9,000) from the GIANT data had no dbSNP151 identifier, and are left as just an rsID. Note that these SNPs are also missing chromosome and position information (not provided in the GIANT data).</p>  <p>Tested_Allele: the allele for which all association statistics are reported</p>  <p>Other_Allele: the other allele at the SNP</p>  <p>Freq_Tested_Allele:&nbsp;frequency of the tested allele</p>  <p>BETA: the effect size of the tested allele</p>  <p>SE: the standard error of the beta</p>  <p>P:&nbsp;the p-value of the SNP, as reported from the inverse variance-weighted fixed effects meta-analysis</p>  <p>N:&nbsp;the total sample size for this SNP</p>  <p>INFO: the imputation quality (info score) of the SNP, as reported by UK Biobank. A number between 0 and 1 indicating quality of imputation (0, poor quality; 1, high quality or genotyped). Note that the summary-level GIANT data does not report info score, so SNPs appearing only in the GIANT analysis do not have info scores.</p>",Benefits and limitations of genome-wide association studies,https://doi.org/10.1038/s41576-019-0127-1,2003,
SPIDER - Lumbar spine segmentation in MR images: a dataset and a public benchmark,10159290,"<p>This is a large publicly available multi-center lumbar spine magnetic resonance imaging (MRI) dataset with reference segmentations of vertebrae, intervertebral discs (IVDs), and spinal canal. The dataset&nbsp;includes 447&nbsp;sagittal T1 and T2 MRI series from 218&nbsp;studies of 218 patients with a history of low back pain. The data was collected from four different hospitals. There is an additional&nbsp;hidden test set, not available here, used in the accompanying SPIDER challenge on spider.grand-challenge.org. We share this data&nbsp;to encourage wider participation and collaboration in the field of spine segmentation, and ultimately improve the diagnostic value of lumbar spine MRI.</p> <p>Which MRI studies are assigned to the training and validation sets can be found in the overview file. This file also provides the biological sex for all patients and the age for the patients for which this was available. It also includes a number of scanner and acquisition parameters for each individual MRI study. The dataset also comes with radiological gradings found in a separate file for the following degenerative changes:</p> <p>1.&ensp;&ensp;&ensp;&ensp;Modic changes (type I, II or III)</p> <p>2.&ensp;&ensp;&ensp;&ensp;Upper and lower endplate changes / Schmorl nodes (binary)</p> <p>3.&ensp;&ensp;&ensp;&ensp;Spondylolisthesis (binary)</p> <p>4.&ensp;&ensp;&ensp;&ensp;Disc herniation (binary)</p> <p>5.&ensp;&ensp;&ensp;&ensp;Disc narrowing (binary)</p> <p>6.&ensp;&ensp;&ensp;&ensp;Disc bulging (binary)</p> <p>7.&ensp;&ensp;&ensp;&ensp;Pfirrman grade (grade 1 to 5).&nbsp;</p> <p>All radiological gradings are provided per IVD level.</p> <div>This dataset, and the associated public benchmark, are described in this paper: <a href=""https://www.nature.com/articles/s41597-024-03090-w"" target=""_blank"" rel=""noopener"">https://www.nature.com/articles/s41597-024-03090-w</a></div> <div>The public segmenation challenge can be found here: <a href=""https://spider.grand-challenge.org/"" target=""_blank"" rel=""noopener"">https://spider.grand-challenge.org/</a></div> <div>&nbsp;</div> <div>When using this dataset, please cite this dataset with the correct DOI, and also cite the afformentioned paper.</div>",Lumbar spine segmentation in MR images: a dataset and a public benchmark,https://doi.org/10.1038/s41597-024-03090-w,47,pdfs/Lumbar_spine_segmentation_in_MR_images__a_dataset_and_a_public_benchmark.pdf
EEG Alpha Waves dataset,2605110,"<p><strong>Summary:</strong></p>  <p>This dataset contains electroencephalographic recordings of 20 subjects in a simple resting-state eyes open/closed experimental protocol. The electroencephalographic headset consisted of 16 electrodes. Data were recorded during a pilot experiment taking place in the GIPSA-lab, Grenoble, France, in 2017 (1). A full description of the experiment may be found at <a href=""https://hal.archives-ouvertes.fr/hal-02086581"">https://hal.archives-ouvertes.fr/hal-02086581</a>. Python code for manipulating the data is available at&nbsp;<a href=""https://github.com/plcrodrigues/Alpha-Waves-Dataset"">https://github.com/plcrodrigues/Alpha-Waves-Dataset</a>&nbsp;. The ID of this dataset is <em>ALPHA.EEG.2017-GIPSA.</em></p>  <p>&nbsp;</p>  <p><strong><span>Full&nbsp;</span>description of the experiment and dataset:</strong>&nbsp;https://hal.archives-ouvertes.fr/hal-02086581</p>  <p>&nbsp;</p>  <p><strong><em>Principal </em></strong><strong><em>Investigators</em>:</strong> Eng. Gr&eacute;goire CATTAN, Eng. Pedro L. C. RODRIGUES</p>  <p><br> <strong><em>Scientific Supervisor</em></strong><strong>:</strong> Dr. Marco Congedo</p>  <p>&nbsp;</p>  <p><strong>ID of the dataset:&nbsp;</strong><em>ALPHA.EEG.2017-GIPSA</em></p>  <p>&nbsp;</p>","Detecting and recognizing driver distraction through various data modality using machine learning: A review, recent advances, simplified framework and open challenges (2014â€“2021)",https://doi.org/10.1016/j.engappai.2022.105309,52,
Regensburg Pediatric Appendicitis Dataset,7711412,"<p>This dataset was acquired in a retrospective study from a cohort of pediatric patients admitted with abdominal pain to Children&rsquo;s Hospital St. Hedwig in Regensburg, Germany. Multiple abdominal B-mode ultrasound images were acquired for most patients, with the number of views varying from 1 to 15. The images depict various regions of interest, such as the abdomen&rsquo;s right lower quadrant, appendix, intestines, lymph nodes and reproductive organs. Alongside multiple US images for each subject, the dataset includes information encompassing laboratory tests, physical examination results, clinical scores, such as Alvarado and pediatric appendicitis scores, and expert-produced ultrasonographic findings. Lastly, the subjects were labeled w.r.t. three target variables: diagnosis (appendicitis vs. no appendicitis), management (surgical vs. conservative) and severity (complicated vs. uncomplicated or no appendicitis).&nbsp;The&nbsp;study was approved by the Ethics Committee of the University of Regensburg (no. 18-1063-101, 18-1063_1-101 and 18-1063_2-101)&nbsp;and was performed following applicable guidelines and regulations.</p>",An interpretable and transparent machine learning framework for appendicitis detection in pediatric patients,https://doi.org/10.1038/s41598-024-75896-y,9,pdfs/An_interpretable_and_transparent_machine_learning_framework_for_appendicitis_det.pdf
Machine Learning Dataset for Poultry Diseases Diagnostics - PCR annotated,5801834,"<p>The dataset of poultry disease diagnostics was annotated using Polymerase Chain Reaction (PCR). Polymerase Chain Reaction (PCR) is a molecular biology technique for rapid diagnostics. We gathered both the fecal images and fecal samples from layers, cross and indigenous breeds of chicken from poultry farms in Arusha and Kilimanjaro regions in Tanzania between September 2020 and February 2021. Each fecal sample collected was coded to its corresponding image during data collection. PCR method is used for detection and identification of pathogens through amplification of DNA sequences unique to the pathogen. We used existing primers from literature to amplify the target DNA/RNA on the poultry fecal samples for PCR. The targets were Coccidiosis, Newcastle disease and Salmonella. We used the primers for PCR diagnostics at the molecular laboratory of the Nelson Mandela African Institution of Science and Technology (NM-AIST). The fecal samples were stored at -80 degrees celsius. The PCR diagnostics were conducted using reagents and kits from Zymo Research and the protocol is summarized in these five stages: 1. DNA sample loading 2. DNA extraction 3. Amplification; 4. Quantification and 5. Detection.</p>  <p>All the PCR annotated fecal images are in the <strong><strong>.zip files</strong></strong>; &ldquo;pcrcocci.zip&rdquo; has 373 images, &ldquo;pcrhealthy.zip&rdquo; has 347 images, &ldquo;pcrsalmo.zip&rdquo; has 349 images, &quot;pcrncd.zip&quot; has 186 images. A total of 1,255 image files are labeled.</p>  <p>The research project is funded by the Organization for Women in Science for the Developing World (OWSD) with Grant Award Number: 4500406715.</p>",Smartphone based detection and classification of poultry diseases from chicken fecal images using deep learning techniques,https://doi.org/10.1016/j.atech.2023.100221,73,
Coronavirus disease (COVID-19) case data - South Africa,7651129,"<p>COVID 19 Data for South Africa created, maintained and hosted by&nbsp;<a href=""https://dsfsi.github.io/"">DSFSI research group</a>&nbsp;at the University of Pretoria</p>  <p><strong>Disclaimer:</strong>&nbsp;We have worked to keep the data as accurate as possible. We collate the COVID 19 reporting data from NICD and South Africa DoH. We only update that data once there is an official report or statement. For the other data, we work to keep the data as accurate as possible. If you find errors let us know.&nbsp;</p>  <p>See original GitHub repo for detailed information&nbsp;<a href=""https://github.com/dsfsi/covid19za"">https://github.com/dsfsi/covid19za</a></p>",Emergence and rapid spread of a new severe acute respiratory syndrome-related coronavirus 2 (SARS-CoV-2) lineage with multiple spike mutations in South Africa,https://doi.org/10.1101/2020.12.21.20248640,1192,pdfs/Emergence_and_rapid_spread_of_a_new_severe_acute_respiratory_syndrome-related_co.pdf
The open D1NAMO dataset: A multi-modal dataset for research on non-invasive type 1 diabetes management,5651217,"<p>The description of the dataset is available at <a href=""https://doi.org/10.1016/j.imu.2018.09.003"">https://doi.org/10.1016/j.imu.2018.09.003</a></p>  <p>The usage of wearable devices has gained popularity in the latest years, especially for health-care and well being. Recently there has been an increasing interest in using these devices to improve the management of chronic diseases such as diabetes. The quality of data acquired through&nbsp;<a href=""https://www.sciencedirect.com/topics/medicine-and-dentistry/wearable-sensor"">wearable sensors</a>&nbsp;is generally lower than what medical-grade devices provide, and existing datasets have mainly been acquired in highly controlled clinical conditions. In the context of the&nbsp;<em>D1NAMO</em>&nbsp;project &mdash; aiming to detect&nbsp;<a href=""https://www.sciencedirect.com/topics/medicine-and-dentistry/glycemic"">glycemic</a>&nbsp;events through non-invasive&nbsp;<a href=""https://www.sciencedirect.com/topics/medicine-and-dentistry/ecg-abnormality"">ECG pattern</a>&nbsp;analysis &mdash; we elaborated a dataset that can be used to help developing health-care systems based on wearable devices in non-clinical conditions. This paper describes this dataset, which was acquired on 20 healthy subjects and 9 patients with type-1 diabetes. The acquisition has been made in real-life conditions with the&nbsp;<em>Zephyr BioHarness 3</em>&nbsp;wearable device. The dataset consists of&nbsp;<em>ECG</em>,&nbsp;<em>breathing</em>, and&nbsp;<em><a href=""https://www.sciencedirect.com/topics/medicine-and-dentistry/accelerometer"">accelerometer</a></em>&nbsp;signals, as well as&nbsp;<em>glucose</em>&nbsp;measurements and annotated&nbsp;<em>food pictures</em>. We open this dataset to the scientific community in order to allow the development and evaluation of diabetes management algorithms.</p>",A Consent Model for Blockchain-Based Health Data Sharing Platforms,https://doi.org/10.1109/access.2020.3014565,118,
DADA2 formatted 16S rRNA gene sequences for both bacteria \& archaea,13984843,"<p><strong><em>This version is to stay up to date with the improvements and increase in 16S rRNA gene sequences (SSU) added to the GTDB release 220.&nbsp; Please read this post for the stats on the updates. </em></strong><strong><em>https://gtdb.ecogenomic.org/stats/r220 </em></strong><strong><em>.</em></strong><strong><em> </em></strong></p> <p><strong><em>There has been no change to the RDP-RefSeq reference database please use previous versions.</em></strong></p> <p><strong><em>If anyone has concerns&nbsp;with MAG extracted 16S rRNA gene contamination concerns, then I suggest that they contact the curators of GTDB themselves because it is outside of my role with these resources designed for DADA2 usage only. </em></strong></p> <p><strong><em>Another concern that was raised was the orientation of the DB sequences, to get past this problem please use the tryRC = TRUE argument in the assignTaxonomy command within DADA2, this will search your ASVs in the reverse complement as well.&nbsp;&nbsp;</em></strong></p> <p>The bacterial and archaeal 16S rRNA gene sequence databases were collated from various sources and formatted to use the ""assignTaxonomy"" command within the DADA2 pipeline. The data was converted to suite DADA2 format by Alishum Ali.</p> <ol> <li>Genome Taxonomy Database (GTDB): The new version of our dada2 formatted GTDB reference sequences now contains 58102 bacteria and 3672 archaea full 16S rRNA gene sequences. If you wonder why there are fewer species with 16S rRNA, that is because some metagenomics-assembled genomes (MAGs) lack the 16S gene and thus cannot be extracted.&nbsp; The database was downloaded from <a href=""https://data.ace.uq.edu.au/public/gtdb/data/releases/release95/"">https://data.ace.uq.edu.au/public/gtdb/data/releases/</a> on 24/10/2024. Please read the release notes and file descriptions.&nbsp;</li> </ol> <p>The formatting to DADA2 was done using simple awk bash scripts. The script takes as input a fasta file and a tab-delimited taxonomy file (slightly edited to remove special characters) and then it outputs a fasta file with all 7 taxonomy ranks separated by "";"" as required for DADA2 compatibility. Additionally, we have concatenated the unique sequence GTDB ID to the species entry (but replaced the ""."" with an "" _"". We see this as an important QC step to highlight the issues/confidence associated with short-read taxonomy assignment at the finer rank levels.</p> <p>Also, this update includes two other files that you can use with the assignTaxonomy and addSpecies commands in DADA2.</p>",Effectiveness of biochar application and bioaugmentation techniques for the remediation of freshly and aged diesel-polluted soils,https://doi.org/10.1016/j.ibiod.2021.105259,23,
