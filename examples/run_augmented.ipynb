{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c73a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import json # Used for saving results\n",
    "from openai import OpenAI\n",
    "from autoddg import AutoDDG\n",
    "from autoddg.utils import get_sample\n",
    "from autoddg.evaluation import BaseEvaluator # Evaluation base class\n",
    "from datetime import datetime\n",
    "import os # For checking file existence\n",
    "\n",
    "# Cell 2: Configuration & Client Initialization\n",
    "# --- LLM Config ---\n",
    "MODEL_CONFIG = {\n",
    "    \"base_url\": \"http://localhost:11434/v1\",\n",
    "    \"api_key\": \"ollama\",\n",
    "    \"model_name\": \"llama3\",\n",
    "    \"evaluation_model_name\": \"llama3\"\n",
    "}\n",
    "\n",
    "# --- Experiment Config ---\n",
    "DATASET_NAME = \"CODE-15%: a large scale annotated dataset of 12-lead ECGs\"\n",
    "DATA_FILE = \"../src/autoddg/related/data/code-15.csv\"\n",
    "PAPER_FILE = \"../src/autoddg/related/papers/code15.pdf\"\n",
    "RESULTS_FILE = \"autoddg_experiment_results.csv\"\n",
    "\n",
    "# --- Define Evaluation Class (Keep as is) ---\n",
    "class Eval(BaseEvaluator):\n",
    "    def __init__(self, model_name: str = MODEL_CONFIG[\"evaluation_model_name\"]):\n",
    "        client = OpenAI(\n",
    "            api_key=MODEL_CONFIG[\"api_key\"], \n",
    "            base_url=MODEL_CONFIG[\"base_url\"]\n",
    "        )\n",
    "        super().__init__(client=client, model_name=model_name)\n",
    "\n",
    "# Initialize Core Tools\n",
    "client = OpenAI(api_key=MODEL_CONFIG[\"api_key\"], base_url=MODEL_CONFIG[\"base_url\"])\n",
    "auto_ddg = AutoDDG(client=client, model_name=MODEL_CONFIG[\"model_name\"])\n",
    "auto_ddg.set_evaluator(Eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f1b0f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Core Profiling ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/nlp_final/lib/python3.12/site-packages/datamart_profiler/core.py:199: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data = data.astype(object).fillna('').astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling Complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Data and Profile\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "sample_df, dataset_sample = get_sample(df, sample_size=100)\n",
    "\n",
    "# Run ALL core profiling steps\n",
    "print(\"--- Running Core Profiling ---\")\n",
    "basic_profile, structural_profile = auto_ddg.profile_dataframe(df)\n",
    "semantic_profile = auto_ddg.analyze_semantics(sample_df)\n",
    "data_topic = auto_ddg.generate_topic(DATASET_NAME, None, dataset_sample)\n",
    "\n",
    "print(\"Profiling Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64edebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define Multiple Related Work Prompts\n",
    "\n",
    "# V0: The original (less effective) prompt\n",
    "PROMPT_V0_ORIGINAL = \"\"\"\n",
    "You are a **Dataset Description Synthesis Expert**. Your task is to extract and synthesize research context *specifically about the dataset* for a search engine description.\n",
    "\n",
    "Extract key research context about the dataset: **{dataset_name}**.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "Your summary MUST cover and integrate the following key research aspects:\n",
    "\n",
    "1. **Research Domain and Applications:** What field or discipline uses this dataset, and what specific research questions or problems does it address?\n",
    "\n",
    "2. **Dataset Usage and Findings:** How did researchers practically use this dataset (e.g., analyses, experiments, modeling), and what were the key results or findings derived from it?\n",
    "\n",
    "3. **Characteristics and Provenance:** Describe how the data was collected or generated, any unique value it provides, and any notable preprocessing or curation steps mentioned.\n",
    "\n",
    "4. **Limitations and Challenges:** Summarize any limitations, challenges, biases, or caveats researchers identified while using this data.\n",
    "\n",
    "**OUTPUT FORMAT:** Synthesize all the extracted information into **one cohesive, natural-language paragraph** (approximately 300-400 words) that describes the research context of the dataset. **DO NOT** use bullet points, section headings (like \"Title,\" \"Abstract,\" \"Results,\" etc.), or lists. The output must be ready to be inserted directly into the final dataset description.\n",
    "\n",
    "**RESEARCH PAPER TEXT:**\n",
    "{paper_text}\n",
    "\"\"\"\n",
    "\n",
    "# V1: The revised, more restrictive prompt (Recommended: 100-150 words)\n",
    "PROMPT_V1_REVISED = \"\"\"\n",
    "You are a concise synthesis expert for a dataset search engine. Your ONLY goal is to extract factual context about the dataset's usage, findings, and limitations from the provided text and convert it into a single, cohesive, non-conversational paragraph.\n",
    "\n",
    "Extract key research context about the dataset: **{dataset_name}**.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "Your summary MUST cover and integrate the following key research aspects:\n",
    "1. Research Domain and Applications.\n",
    "2. Dataset Usage and Findings.\n",
    "3. Characteristics and Provenance.\n",
    "4. Limitations and Challenges.\n",
    "\n",
    "OUTPUT FORMAT: Synthesize all the extracted information into **one cohesive, natural-language paragraph** (approximately 100-150 words). DO NOT use bullet points, section headings, or lists.\n",
    "\n",
    "RESEARCH PAPER TEXT:\n",
    "{paper_text}\n",
    "\"\"\"\n",
    "\n",
    "# Store prompts in a dictionary for easy iteration\n",
    "RELATED_WORK_PROMPTS = {\n",
    "    \"V0_Original\": PROMPT_V0_ORIGINAL,\n",
    "    \"V1_Revised\": PROMPT_V1_REVISED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95398a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Logging Function (Updated to use your 3 metrics)\n",
    "\n",
    "def parse_scores(raw_score_text: str) -> dict:\n",
    "    \"\"\"Parses the 'Metric: Score' string output into a dictionary.\"\"\"\n",
    "    scores = {}\n",
    "    lines = raw_score_text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            try:\n",
    "                # Store the key in a normalized way (lowercase, no spaces)\n",
    "                scores[key.strip().lower()] = int(value.strip())\n",
    "            except ValueError:\n",
    "                # Handle cases where the value isn't an integer\n",
    "                pass\n",
    "    return scores\n",
    "\n",
    "def log_result(\n",
    "    prompt_name, \n",
    "    description_type, \n",
    "    description, \n",
    "    raw_scores, # Now accepts the raw text string\n",
    "    file_path=RESULTS_FILE\n",
    "):\n",
    "    \"\"\"Logs the results of a single test run to a CSV file.\"\"\"\n",
    "    \n",
    "    # NEW STEP: Parse the raw score string\n",
    "    parsed_scores = parse_scores(raw_scores)\n",
    "    \n",
    "    # Extract the three core metrics for CSV columns\n",
    "    completeness = parsed_scores.get('completeness', 0)\n",
    "    conciseness = parsed_scores.get('conciseness', 0)\n",
    "    readability = parsed_scores.get('readability', 0)\n",
    "    \n",
    "    # Storing the full dictionary for detail (helpful if the LLM sometimes adds a new metric)\n",
    "    raw_scores_json = json.dumps(parsed_scores) \n",
    "    \n",
    "    new_row = {\n",
    "        'Test_ID': f\"{description_type}-{datetime.now().strftime('%H%M%S')}\",\n",
    "        'Dataset_Name': DATASET_NAME,\n",
    "        'Prompt_Type': prompt_name,\n",
    "        'Description_Source': description_type,\n",
    "        'Description_Text': description.replace('\\n', ' '), \n",
    "        'Completeness_Score': completeness,\n",
    "        'Conciseness_Score': conciseness,\n",
    "        'Readability_Score': readability,\n",
    "        'Raw_Scores_JSON': raw_scores_json, \n",
    "        'Evaluation_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    }\n",
    "\n",
    "    df_new = pd.DataFrame([new_row])\n",
    "    \n",
    "    # Check if file exists to decide whether to write header\n",
    "    header_needed = not os.path.exists(file_path)\n",
    "    \n",
    "    # Append to CSV\n",
    "    df_new.to_csv(file_path, mode='a', header=header_needed, index=False)\n",
    "    print(f\"Logged {description_type} with Prompt {prompt_name} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bef8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Baseline (Vanilla) Test ---\n",
      "Baseline Scores: Here are my scores for the given dataset description based on the Evaluation Criteria:\n",
      "\n",
      "Evaluation Form (scores ONLY):\n",
      "\n",
      "Completeness: 9\n",
      "Conciseness: 8\n",
      "Readability: 9\n",
      "Logged Vanilla_AutoDDG with Prompt N/A to autoddg_experiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Run and Log Baseline (Vanilla) Description\n",
    "\n",
    "print(\"\\n--- Running Baseline (Vanilla) Test ---\")\n",
    "prompt_baseline, description_baseline = auto_ddg.describe_dataset(\n",
    "    dataset_sample=dataset_sample,\n",
    "    dataset_profile=basic_profile,\n",
    "    use_profile=True,\n",
    "    semantic_profile=semantic_profile,\n",
    "    use_semantic_profile=True,\n",
    "    data_topic=data_topic,\n",
    "    use_topic=True,\n",
    "    use_related_profile=False  # Vanilla\n",
    ")\n",
    "\n",
    "baseline_scores = auto_ddg.evaluate_description(description_baseline)\n",
    "print(f\"Baseline Scores: {baseline_scores}\")\n",
    "\n",
    "log_result(\"N/A\", \"Vanilla_AutoDDG\", description_baseline, baseline_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f423264c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 43 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Augmented Test with Prompt: V0_Original ---\n",
      "Reading PDF from: ../src/autoddg/related/papers/code15.pdf\n",
      "Successfully extracted text from 10 pages (total: 10 pages)\n",
      "Total characters extracted: 55673\n",
      "Extracting related work profile for dataset: CODE-15%: a large scale annotated dataset of 12-lead ECGs\n",
      "Sending 57116 characters to LLM...\n",
      "Successfully extracted profile (1563 characters)\n",
      "Related Work Summary: As a Dataset Description Synthesis Expert, I will extract and synthesize the research findings related to the comparison of dataset description synthe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 43 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Scores (V0_Original): Here are my scores for the given dataset description based on the Evaluation Criteria:\n",
      "\n",
      "Evaluation Form (scores ONLY):\n",
      "\n",
      "Completeness: 9\n",
      "Conciseness: 8\n",
      "Readability: 9\n",
      "Logged Augmented_AutoDDG with Prompt V0_Original to autoddg_experiment_results.csv\n",
      "\n",
      "--- Running Augmented Test with Prompt: V1_Revised ---\n",
      "Reading PDF from: ../src/autoddg/related/papers/code15.pdf\n",
      "Successfully extracted text from 10 pages (total: 10 pages)\n",
      "Total characters extracted: 55673\n",
      "Extracting related work profile for dataset: CODE-15%: a large scale annotated dataset of 12-lead ECGs\n",
      "Sending 56479 characters to LLM...\n",
      "Successfully extracted profile (941 characters)\n",
      "Related Work Summary: The synthesis expert has extracted the following factual context:\n",
      "\n",
      "* The dataset search engine is comparing the performance of different machine learn...\n",
      "Augmented Scores (V1_Revised): Here are the scores:\n",
      "\n",
      "Evaluation Form (scores ONLY):\n",
      "\n",
      "Completeness: 9\n",
      "Conciseness: 8\n",
      "Readability: 7\n",
      "Logged Augmented_AutoDDG with Prompt V1_Revised to autoddg_experiment_results.csv\n",
      "\n",
      "All experiments complete. Results saved to: autoddg_experiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Run and Log Augmented Descriptions for Multiple Prompts\n",
    "\n",
    "for prompt_name, extraction_prompt in RELATED_WORK_PROMPTS.items():\n",
    "    print(f\"\\n--- Running Augmented Test with Prompt: {prompt_name} ---\")\n",
    "    \n",
    "    # Step A: Analyze related work using the current prompt\n",
    "    related_profile = auto_ddg.analyze_related(\n",
    "        pdf_path=PAPER_FILE,\n",
    "        dataset_name=DATASET_NAME,\n",
    "        extraction_prompt=extraction_prompt,\n",
    "        max_pages=10\n",
    "    )\n",
    "    print(f\"Related Work Summary: {related_profile['summary'][:150]}...\")\n",
    "\n",
    "    # Step B: Generate description with the new related profile\n",
    "    prompt_augmented, description_augmented = auto_ddg.describe_dataset(\n",
    "        dataset_sample=dataset_sample,\n",
    "        dataset_profile=basic_profile,\n",
    "        use_profile=True,\n",
    "        semantic_profile=semantic_profile,\n",
    "        use_semantic_profile=True,\n",
    "        data_topic=data_topic,\n",
    "        use_topic=True,\n",
    "        related_profile=related_profile,\n",
    "        use_related_profile=True # Augmented\n",
    "    )\n",
    "    \n",
    "    # Step C: Evaluate and Log\n",
    "    augmented_scores = auto_ddg.evaluate_description(description_augmented)\n",
    "    print(f\"Augmented Scores ({prompt_name}): {augmented_scores}\")\n",
    "    \n",
    "    log_result(prompt_name, \"Augmented_AutoDDG\", description_augmented, augmented_scores)\n",
    "    \n",
    "print(\"\\nAll experiments complete. Results saved to:\", RESULTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9001d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
